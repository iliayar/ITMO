#+setupfile: setup.org
#+TITLE: Лекция 1
#+date: 2 сентября

* Введение
#+NAME: Структура систем передачи информации
#+ATTR_LATEX: :width 0.3\textwidth
[[file:1.1.png]]

** Модулятор
#+NAME: Простейшие методы модуляции_опр
#+begin_definition org
Передаваемый сигнал равен
\[ x(t) = \sum_i S_{x_i} (t - iT) \],
где \(x_i\) -- передаваемые символы, \(T\) -- продолжительность символьного интервала
#+end_definition

#+NAME: Простейшие методы модуляции_1
#+begin_examp org
\(M\)-ичная амплитудно-импульсная модуляция
\[ S_i(t) = \alpha(2i + 1 - M)g(t)\sin(2\pi f t) \],
где \(g(t)\) -- сигнальный импульс (например, единичный импульс продолжительностью \(T\)), \(f\) -- несущая частота,
\(\alpha\) -- коэффицент, определяющий энергию передаваемого сигнала
#+end_examp
#+NAME: Простейшие методы модуляции_2
#+begin_examp org
Модель канала в непрервном времени \(y(t) = x(t) + \eta(t)\)
#+end_examp
#+NAME: Простейшие методы модуляции_3
#+begin_examp org
Модель канала в дискретном времени \(y_i = \alpga(2x_i + 1 - M) + \eta_i\)
#+end_examp
#+NAME: Простейшие методы модуляции_def1
#+begin_definition org
\(\eta_i \sim \mathcal{N}(0, \sigma^2)\) -- канал с *аддитивным белым гауссовским шумом*
#+end_definition

** Приемник
#+NAME: Критерии идеального наблюдателя и максимума правдоподобия_rem
#+begin_rem org
Приемник наблюдает на выходе канала вектор \(y = \begin{pmatrix} y_0 & \dots & y_{n - 1} \end{pmatrix} \). \\
Канал характеризуется условным распределением \(p_{Y|X}(y|x)\), где \(X, Y\) -- случайные величины, соответствующие векторам переданных и принятых символов.
Если выход канала -- непрерывная случайная величина, \(p_{Y|X}(y|x)} \) -- условная плотность вероятности.
Приемник реализует некоторое разбиение векторного пространства на решающие области \(R_x : y \in R_x \implies \hat{x} = x\)
#+end_rem

#+NAME: Вероятность ошибки
#+begin_definition org
*Вероятность ошибки*
\[ P_e = \int_{\R^N} p_e(y)p_Y(y) dy = \sum_x \int_{R_x} p_e(y) p_Y(y) dy = \]
\[ = \sum_x\int_{R_x} (1 - p_{X|Y}(x|y)) p_Y(y) dy = 1 - \sum_x\int_{R_x}p_{X|Y}(x|y) p_Y(y) dy \]
#+end_definition

Хотим минимизировать \(P_e\):

#+NAME: Критерии идеального наблюдателя
#+begin_definition org
Критерий максимума апостериорной вероятности (*критерий идеального наблюдателя*)
\[ R_x = \{y|p_{X|Y}(x|y) > p_{X|Y}(x'|y), x' \neq x \} = \{ y | P_X(x) p_{Y|X}(y|x) > P_X (x') p_{Y|X}(y|x'), x' \neq x \} \]
#+end_definition

#+NAME: Критерии максимума правдоподобия
#+begin_definition org
*Критерий максимума правдоподобия*
\[ R_x = \{y | p_{Y|X}(y|x) > p_{Y|X}(y | x'), x' \neq x\} \]
#+end_definition

#+NAME: Вероятность ошибки сигналов 2-АМ в случае канала с АБГШ
#+begin_examp org
2-ичная амплитудно-импульсная модуляция (2-АМ). Пусть \(y_i = \alpha (2x_i - 1) + \eta_i, \eta_i \sim \mathcal{N}(0, \sigma^2), x_i \in \{0, 1\}\). Тогда:
\[ p_{Y|X}(y|x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y - \alpha (2x - 1))^2}{2\sigma^2}} \]
Применим критерий максимального правдоподобия:
\[ R_0 = \{y | y < 0\}, R_1 = \{y | y \ge 0\} \]
Вычислим вероятность ошибки:
\[ P_e = P_X(0)P\{Y \ge 0 | X = 0\} + P_X(1)P\{Y  < 0 | X = 1\} = \dots = \int^\infty_{\frac{\alpha}{\sigma}} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2\sigma^2}} dy = Q\left(\frac{\alpha}{\sigma}\right) = \]
\[ = \frac{1}{2}\mathop{erfc} \left(\frac{\alpha}{\sqrt{2\sigma}}\right) \]
#+end_examp

#+NAME: Отношение сигнал/шум на бит и на символ
#+begin_rem org
Значение сигнала это обычно уровень напряжения. Как мы знаем мощность \(P = \frac{U^2}{R}\). Мы хотим минимизировать мощность, чтобы экономить электроэнергию.
Мощность сигнала суть случайная величина с матожиданием, пропорциональным \(E_S = \alpha^2\). Мощность белого шума не зависит от частоты и пропорциональна \(\sigma^2 = \frac{N_0}{2}\).
Если же шум зависит от частоты, то он называется розовым или голубым.

Соотношение мощностей сигнал/шум на символ это \(\frac{E_S}{N_0}\), обычно измеряемое в децибелах, т.е. \(10 \log_{10} \frac{E_S}{N_0}\). Однако нас интересуют не символы,
а биты и тогда соотношение сигнал/шум на бит это \(\frac{E_S}{RN_0}\), где \(R\) -- количество бит информации, представленных одним символом.
#+end_rem

* Понятие кода
#+NAME: Блоковые коды и их параметры_code_def
#+begin_definition
*Код* -- множество допустимых последовательностей символов алфавита \(X\), как конечных так и бесконечных
#+end_definition
#+begin_rem
На практике ограничиваются послеловательностями длины \(n\)
#+end_rem
#+begin_rem
Не всякая последовательность символов из \(X\) является кодовой
#+end_rem

#+NAME: Блоковые коды и их параметры_coder_def
#+begin_definition
Кодер -- устройство, реализующее отображение информационных последовательностей символов алфавита \(B\)  в кодовые
#+end_definition
#+begin_rem
Различным информационным последовательностям сопоставляются разсличные кодовые последовательности
#+end_rem
#+begin_definition
*Скорость кода* -- отношение длин информационной и кодовой последовательностей
#+end_definition

#+NAME: Блоковые коды и их параметры_decoder
#+begin_definition
*Декодер* -- устройство, восстанавливающее по принятой последовательности символов /наиболее вероятную/ соответствующую ей кодовую (или информационную) последовательность
#+end_definition
#+begin_rem
Под наиболее вероятным подразумевается критерии иделального наблюдателя и максимального правдоподобия
#+end_rem

** Теорема кодирование
Пусть для передачи используеся код \(\Co \subset X^n\) длины \(n\), состоящий из \(M\) кодовых слов, выбираемых с одинаковой вероятностью
#+ATTR_LATEX: :options [Обратная]
#+begin_theorem
Для дискретного постоянного канала с пропускной способностью \(C\) для любого \(\delta > 0\) существует \(\varepsilon > 0\) такое, что для любого кода со скоростью
\(R > C + \delta\) средняя вероятность ошибки \(\bar{P}_\varepsilon > \varepsilon\)
#+end_theorem
#+begin_rem
Постоянный канал -- статистические свойства со временем не меняются \\
Дискретный канал -- вход и выход дискретные
#+end_rem
#+begin_rem
Здесь говориться о том канал характеризуется величиной \(C\). Если попробуем передать данные с большей пропускной способностью, то вероятность ошибки будет ограничена снизу.
#+end_rem
#+ATTR_LATEX: :options [Прямая]
#+begin_theorem
Для дискретного постоянного канала с пропускной способностью \(C\) для любых \(\varepsilon, \delta > 0\) существует достаточно большое число \(n_0 > 0\), такое что
для всех натуральных \(n \ge n_0\) существует код длиной \(n\) со скоростью \(R \ge C - \delta\), средняя вероятность ошибки которого \(P_\varepsilon \le \varepsilon\)
#+end_theorem



** Пропускные способности каналов
#+begin_definition
Двоично симметричный канал: \(X, Y \in \{0, 1\}, p_{Y|X}(y|x) = \begin{cases} p, & y \neq x \\ 1 - p, & y = x \end{cases}\)
\[ C_{\text{BSC}} = 1 + p\log_2p + (1 - p)\log_2(1 - p) \]
#+end_definition

#+begin_definition org
Идеальный часточно ограниченный гауссовский канал \(y(t) = x(t) + \eta(t)\), \(\eta(t)\) -- гауссовский случайный процесс, спектральная плотность мощности которого
равна \(S(f) = \begin{cases} \frac{N_0}{2}, & -W < f < W \\ 0 & \text{иначе} \end{cases} \)
\[ C_{\text{AWGN}} = W \log_2 \left( 1 + \frac{E_s}{WN_0} \right) \]
#+end_definition


** Мягкое и жесткое декодирование
Канал с аддитивным белым гауссовским шумом: \(y_i = (2x_i - 1) + \eta_i, x_i \in \{0, 1\}\)
#+begin_definition
Мягкое декодирование: декодер непосредственно использует \(y_i\)
#+end_definition
#+begin_definition
Жесткое декодирование: декодер использует оценки \(\hat{x}_i\)
#+end_definition


** Спектральная эффективность
#+begin_definition org
*Спектральная эффективность* кодирования \(\beta = \frac{R}{W} \ \left[ \frac{\text{бит}}{\text{сГц}} \right]\)
#+end_definition

