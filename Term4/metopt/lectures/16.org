#+TITLE: Лекция 16
#+SETUPFILE: setup.org


* Квазиньютоновские методы
- объединение достоинств
  1. наискорейшего спуска
  2. метода Ньютона
- не требует обращения матрицы \(H\)
- сохраняют высокую сходимость итерационной последовательности
** Общий вид релаксационной последовательности
\[ x^k = x^{k - 1} + \alpha_k p^k \]
\[ p^k = G_k w^k \quad k \in N \addtag\label{16_1} \]
\[ w^k = -\nabla f(x^{k - 1}) \]
\(G_k\) --- положительно определенная матрица \((n \times n)\) специального вида \\
Поскольку \(G_k > 0\), то \(p^k\) задает направление спуска:
\[ \pair{\nabla f(x^{k - 1}), p^k} = -\pair{w^k, G_k w^k} = -\pair{G_k w^k, w^k} < 0 \]
*** Вычисление матриц \(G_k\)
\[ \{G_k\} \xrightarrow[k \to \infty]{} H^{-1}(x^*) \]
, где \(x^*\) --- точка минимума \\
На первой итерации полагают \(G_1 = I\) --- выполняется градиентный спуск
*** Параметр \(\alpha_k\) в \ref{16_1}
- задать \(\alpha_k = 1\)
- применить дробление шага
- использовать исчерпывающий спуск в направлении \(p^k\) (чаще всего используется на практике)
Поскольку \(\{G_k\} \to \{H^{-1}(x^*)\}\), то \(G_k\) на завершающей стадии поиска близка к \(H^{-1}(x^{k - 1})\), используемой в методе Ньютона \(\implies\) квазиньютоновские метода сохраняют высокую скорость сходимости, присущую методу Ньютона. Таким образом \(G_k\) должна быть близка к \(H^{-1}(x^*)\) и \(G_k\) получают путем аппроксимации \(H^{-1}(x{k - 1})\) \\
Выбор удачной аппроксимации может существенно сократить объем вычислений по сравнению с обращением матрицы \(H\), тем самым упростить процедуру построения направления спуска \(p^k\) --- *идея квазиньютоновских методов* \\
Каждую \(G_{k + 1}\) строят, корректирую ранее вычисленную матрицу \(G_k\)
\[ G_{k + 1} = G_k + \Delta G_k \quad k \in \N \addtag\label{16_2} \]
\(\Delta G_k\) --- положительно определенная матрица \((n \times n)\) --- *поправочная матрица* \\
Способ выбора \(\Delta G_k\) определяет конкретный квазиньютоновский метод. На первой итерации \(G_1 = I\)
#+begin_examp org
Пусть
\[ f(x) = \frac{1}{2} \pair{Ax, x} + \pair{b, x} \]
, где \(A\) --- положительно определенная \\
Тогда \(\forall x: A= H\), \(\nabla f(x) = Ax + b\) \\
Введем: \(\Delta w^k = w^k - w^{k - 1}\), \(\Delta x^k = x^k - x^{k - 1}\), следовательно
\[ \Delta w^k = \nabla f(x^{k - 1}) - \nabla f(x^k) = A(x^{k - 1} - x^k) = - A\Delta x^k \]
Таким образом, для квадратичной функции
\[ A^{-1} \Delta w^k = - \Deta x^k, k \in \N \addtag\label{16_3} \]
Иначе в виду \ref{16_3} для общего случая потребуется, чтобы матрица \(G_{k + 1}\) удовлетворяли условию
\[ G_{k + 1}\Delta w^k = -\Delta x^k \quad k \in \N \addtag\label{16_4} \] --- *квазиньютоновское условие*
#+end_examp
Для поправочных матриц условие \ref{16_4} примет вид
\[ \Delta G_k \Delta w^k = - \Delta x^k - G_k \Delta w^k \]
Этому условию удовлетворяют матрицы
\[ \Delta G_k = -\frac{\Delta x^k y^T}{\pair{\Delta w^k, y}} - \frac{G_k \Delta w^k z^T}{\pair{\Delta w^k, z}} \addtag\label{16_5} \]
\(y, z \in E_n\) --- выбираются произвольно, \(\Delta G_k\) --- симметричные матрицы(необходимое условие, чтобы аппроксимация \(H^{-1}\) давала симметричную матрицу) \\
Пусть в \ref{16_5} \(y = \Delta x^k\), \(z = G_k w^k\) и учитывая \ref{16_2}, получим
\[ G_{k + 1} = G_k - \frac{\Delta x^k (\Delta x_k)^T}{\pair{\Delta w^k, \Delta x^k}} - \frac{G_k \Delta w^k (\Delta w^k)^T G_k^T}{\pair{G_k \Delta w^k, \Delta w^K}} \addtag\label{16_6} \]
Формула \ref{16_6} в сочетании с исчерпывающим спуском по направлениям векторов \(p^k\) --- *метод Давидона-Флетчера-Пауэлла* (ДФП-метод)
- на первой итерации: \(G_1 = I\), задается \(x^0\)
  \[ w^1 = -\nabla f(x^0) \quad p^1 = w^1 \]
  \[ \alpha_1 = \min_\alpha f(x^0 + \alpha p^1) \]
  \[ x_1 = x^0 + \alpha p^1 \quad \Delta x^1 = x^1 - x^0 \]
- \(\forall k > 1\)
  \[ w^k = -\nabla f(x^{k - 1}) \]
  \[ \Delta w^k = w^k - w^{k - 1} \]
  \[ v^k = G_{k - 1}\Delta w^k \]
  \[ G_k = G_{k - 1} - \frac{\Delta x^{k - 1}(\Delta x^{k - 1})^T}{\pair{\Delta w^k, \Delta x^{k - 1}}} - \frac{v^k(v^k)^T}{\pair{v^k, \Delta w^{k}}} \]
  \[ p^k = G_k w^k \]
  \[ \alpha_k = \min_\alpha f(x^{k - 1} + \alpha_k p^k) \]
  \[ x^k = x^{k - 1} + \alpha_k p^k \quad \Delta x^k = x^K - x^{k - 1} \]
  условие останова \(\norm{\Delta x^k} < \varepsilon\)
** Свойства метода ДФП
\beginproperty
#+begin_property org
\(G_k\) по условию \ref{16_6} сохраняет положительную определенность: если \(G_k > 0\), то \(G_{k + 1} > 0\) (при условии \((\Delta x^k)^T \Delta \omega^k > 0\))
#+end_property
#+begin_property org
Если в \ref{16_6} \(G_k\) --- симметричная, то \(G_{k + 1}\) --- симметричная
#+end_property
#+begin_property org
При минимизации квадратичная функция с положительно определенной матрицей \(A\) метод ДФП сводится к методу сопряженных градиентов, т.к. первые \(n\) векторов \(p^k\) (задающих направление спуска) являются сопряженными относительно матрицы \(A\). Следовательно ДФП-метод не более чем за \(n\) итераций дает точное решение квадратичной функции
#+end_property
#+begin_property org
Матрицы \(G_k\), вычисленные по ДФП-методу, связаны равенством
\[ G_k A p^i = p^i \quad i = \overline{1, k} \quad k = \overline{1, n} \]
При \(k = n\) следует, что \(n\) векторов \(p^i\) являются собственными для симметричной матрицы \(G_n A\), а соответствующие собственные значения равны 1. Следовательно \(G_n A = I\) и \(G_n = A^{-1}\), т.е. \(G_n\) оказывается обратной к матрице Гессе \(H(x^*) = A\) квадратичной функции в точке \(x^*\)
#+end_property
#+begin_property org
Если \(f(x)\) не является квадратичной, то ДФП-метод не позволяет найти минимум за конечное число итераций.

Чтобы ослабить влияние накапливаемых погрешностей на сходимость итерационной последовательности и уменьшить вероятность появления после очередных \(n\) итераций линейно зависимых направлений спуска, в ДФП-методе применяют процедуру `обновления` алгоритма: через каждые \(n\) итераций в качестве \(G_n\) используют \(G_n = I\)
#+end_property
#+begin_property org
Если \(f(x)\) --- квадратичная функция с \(H > 0\), то ДФП-метод (с точным одномерным поиском) после \(n\) итераций дает
\[ H^{-1} = \sum_{i = 1}^n \frac{\Delta x^i (\Delta x^i)^T}{(\Delta x^i)^T\cdot \Delta w^i} \]
ДФП-метод очень чувствителен к точности одномерного поиска 
#+end_property
** Метод Бройдена-Флетчера-Шенно (БФШ-метод)
На первой итерации \(G_1 = I\), затем
\[ G_{k + 1} = G_k - \frac{\Delta x^k (\Delta x^k)^T}{\pair{\Delta w^k, \Delta x^k}} - \frac{G_k \Delta w^k(\Delta w^k)^T G_k^t}{\rho_k} + \rho_kr^k(r^k)^T \addtag\label{16_7} \]
\[ r^k = \frac{G_k \Delta w^k}{\rho_k} - \frac{\Delta x^k}{\pair{\Delta x^k, \delta w^k}} \]
\[ \rho_k = \pair{G_k \Delta w^k, \Delta w^k} \]
** Метод Пауэлла
На первой итерации \(G_1 = I\), затем
\[ G_{k + 1} = G_k - \frac{\Delta \tilde{x}^k (\Delta \tilde{x}^k)^T}{\pair{\Delta w^k, \Delta \tilde{x}^k}} \quad k \in \N \addtag\label{16_8} \]
\[ \Delta \tilde{x}^k = \Delta x^k + G_k\Delta w^k \]
** Способы построения \(G_k\)
- сохраняет свойство положительно определенности матрицы
- последовательность \(\{G_k\}\xrightarrow[k \to \infty]{} H^{-1}(x^*)\)
* Сравнение методов
Методы:
- нулевого порядка (не используют значения производных)
- первого порядка (наискорейший спуск, сопряженные градиенты, квазиньютоновские методы)
- второго порядка(метод Ньютона и его модификации, метод Марквардта)
** Сравнение по скорости
Методы порождают последовательность \(\{x^k\}\):
\[ \lim_{k \to \infty} x^k = x^* \]
тогда по свойству предела
\[ \norm{x^{k + 1} - x^*} < \norm{x^k - x^*} \]
поэтому
\[ \frac{\norm{x^{k + 1} - x^*}}{\norm{x^k - x^*}} < 1 \]
В этом случае говорят о сходимости метода
#+begin_examp org
Если \(f(x)\) --- непрерывно дифференцируемая функция и \(f(x) \to +\infty\) при \(\norm{x} \to +\infty\), то для любого \(x^0\) метод наискорейшего спуска сходится к стационарной точке \(f(x)\)
#+end_examp
- Глобальная сходимость методов сопряженных градиентов обеспечена с использованием рестартов. Тогда их глобальная сходимость следует из глобальной сходимости метода наискорейшего спуска
- Метод Ньютона не обладает свойством глобальной сходимости. Если \(x^0\) далека от \(x^*\), то метод не сходится

  Но метод Ньютона с одномерным поиском и направлением спуска и метод Марквардта обладают глобальной сходимостью
- Квазиньютоновсие методы ДФП и БФШ обладают глобальной сходимостью в случае применения рестартов
** Оценка эффективности
Эффективность зависит от числа итераций \\
Если
\[ \lim_{k \to \infty}\frac{\norm{x^{k + 1} - x^*}}{\norm{x^k - x^*}} = \alpha < 1 \quad \alpha \neq 0 \]
,то сходимость линейная. Если
\[ \lim_{k \to \infty}\frac{\norm{x^{k + 1} - x^*}}{\norm{x^k - x^*}} = 0 \]
, то сходимость сверхлинейная. Если \(\exists \gamma > 1\):
\[ \lim_{k \to \infty}\frac{\norm{x^{k + 1} - x^*}}{\norm{x^k - x^*}^\gamma} < +\infty \]
то сходимость порядка \(\gamma\). В частности при \(\gamma = 2\) --- квадратичная сходимость
- Методы сопряженных градиентов имеют сверхлинейную скорость сходимости по \(n\) шагам.

  Если \(H(x)\) удовлетворяет условию Липшица, то эти методы имеют квадратичную сходимость по \(n\) шагам
- Квазиньютоновские методы ДФП и БФШ имеют сверхлинейную скорость сходимости

  Если \(H(x)\) удовлетворяет условию Липшица, то эти методы имеют квадратичную скорость сходимости

  Это показывает преимущество квазиньютоновских методов перед методами сопряженных градиентов, которые требуют \(\approx\) в \(n\) раз больше итераций для одного и того же асимптотического поведения. Однако это преимущество сильно снижается загрузкой памяти \(\sim n^2\) и объемом промежуточных матричных вычислений \(\sim n^2\)
- Метод Ньютона имеет квадратичную локальную сходимость, если \(H(x)\) удовлетворяет условию Липшица

  Следовательно в малой окрестности \(x^*\) метод Ньютона при указанных предположениях целевой функции \(f(x)\) сходится быстрее остальных методов.
