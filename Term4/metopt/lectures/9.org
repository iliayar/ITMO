#+LATEX_CLASS: general
#+TITLE: Лекция 9
#+AUTHOR: Ilya Yaroshevskiy

* Метод сопряженных градиентов
\[ x^{k + 1} = x^k + \alpha_k p^k \quad k = 0,1,\dots \addtag\label{17_9}\]
\[ p^k = - \nabla f(x^*) \]
Направление убывания может носить зигзагообразный характер. Будем находить вектор \(p^k\) не только через антиградиент, но и через \(p^{k - 1}\).

\[ p^{k + 1} = - \nabla(x^{k + 1}) + \beta_k p^k \addtag\label{18_9}\]
\(\beta_k\) выбираются так, чтобы получалась последовательность \(A\)-ортогональных векторов \(p^0, p^1, \dots\). Из условия:
\[ (Ap^{k + 1}, p^k) = 0 \]
\[ \beta_k = \frac{(A \nabla f(x^{k + 1}), p^k)}{(Ap^k, p^k)} \addtag\label{19_9}\]
Для квадратичных функция:
\[ \alpha_k = -\frac{(\nabla f(x^k), p^k)}{(Ap^k, p^k)} \addtag\label{20_9}\]

Утвержение: итерационный процесс, который описывается формулами \ref{17_9}, \ref{18_9}, \ref{19_9}, \ref{20_9}, с положтельно определенной симметричной матрицей \(A\) дает точки \(x^0,\dots,x^k\) и векторы, такие что если \(\nabla f(x^i) \neq 0\), \(0 \le i < k \le n - 1\), то векторы \(p^0,\dots,p^k\) --- \(A\)-ортогональны, а градиенты \(\nabla f(x^0), \dots, \nabla f(x^i)\) --- взаимно ортогональны.

Т.к. \(p^k\) в \ref{18_9} \(A\)-ортогональны, то метод гарантирует нахождение точки минимума сильно выпуклой квадратичной функции _не более чем за \(n\) шагов_
\[ x^{k + 1} = x^k + \alpha_k p^k \quad k = 0,1,\dots\quad x^0 \in E_k\quad p^0 = - \nabla f(x^0) \addtag\label{21_9}\]
\[ f(x^k + \alpha_k p^k) = \min_{\alpha > 0} f(x^k + \alpha p^k)\quad k = 0,1, \dots \addtag\label{22_9}\]
\[ p^{k + 1} = -\nabla f(x^{k + 1}) + \beta_k p^k\quad k = 0,1,\dots \addtag\label{23_9}\]
\[ \beta_k = \frac{\Vert \nabla f(x^{k + 1}) \Vert ^2}{\Vert \nabla f(x^k) \Vert ^2} \addtag\label{24_9}\]

Точное определение \(\alpha_k\) возможно только в редких случаях, т.к. \(p^k\) могут быть не \(A\)-ортогональными. В этом методе используется следующий практический прием: через \(N\) шагов производится обновление метода, т.е. \(\beta_{m\cdot N} = 0\quad m = 1, 2, \dots\), где \(m\cdot N\) --- момент обновления метода(рестарта), часто полагают \(N = n\) --- размерность пространства \(E_n\). Ретарт необходим для устранения накопленной погрешности метода, из-за которой вектора \(p^k\) перестанут указывать на направление убывания функции \(f(x)\) 

Если функция хорошо апроксимируется квадратичной функции, то метод сопряженных градиентов даст маленькое количество шагов
* Метод стохастического градиентного спуска
Этот метод по большей части связан с большими выборками. Обычные методы пострадют, из-за дорогого вычисления функции на большом наборе данных.

Наборы разбивают на \(K\) тренировочных наборов, части тренировочных наборов размера \(M\) называют minibatch. Тогда набор можно предсавить как:
\[ X^{(k)} = \{x_i | i = M_k,\dots,(M_k + M - 1)\} \]
\[ Y^{(k)} = \{y_i | i = M_k,\dots,(M_k + M - 1)\} \]
Определяют некоторую функцию, которую будем оптимизировать. Для каждого набора она будет выглядеть так:
\[ L^{(k)}(\omega) = \sum_{i = 0}^M L(\omega, x_{M_k + i}, y_{M_k + i}) \quad k = 0,\dots,(K - 1)\]
, где \(\omega\) --- точк минимума

Когда определяем функцию для каждого набора, каждая составляющая \(\omega\) будет находится на мини итерации:
\[ \omega_p^{(k + 1)} = \omega_p^{(k)} - \eta \cdot \nabla L^{(k)}(\omega_p^{(k)}) \quad k = 0,\dots,(K - 1)\]
\[ \omega^{(0)}_{p + 1} = \omega^{(k)}_p \]
Большая итерация: \(p = 0,1,\dots\) завершается когда проходим весь набор миниитераций. Такая большая итерация называется эпохой. Когда переходим к следующей эпохе, перемешивает тренировочный набор. В результате пермешивания, элементы будут попадать в разные minibatch'и на каждой эпохе.

** Adagrad (модификация)
Предлагается использовать разные \(\eta\), для каждого minibatch'а.
\[ \eta_p = (\eta^{(1)}_p,\dots,\eta_p^{(d)}) \]
\[ \eta_0 = \const \quad \eta^{(i)}_0 = \eta \quad i = 1,\dots,d\]
\[ \omega_p = (\omega^{(1)}_p,\dots,\omega^{(d)}_p) \]
\[ \nabla L(\omega_p) = (g^{(1)}_p,\dots,g^{(d)}_p) \]
Определим вспомогательный вектор:
\[ G^{(i)}_p = (G^{(1)}_p,\dots,G^{(d)}_p) \color{red}???\color{black} \]
\[ G^{(i)}_p = \sum_{j = 1}^p (g_i^{(i)})^2 \quad i = 1,\dots,d\]
\[ \eta_p^{(i)} = \frac{\eta}{\sqrt{G^{(i)}_p + e}} \]
, где \(e\) --- коэффицент \(\sim 1e-8\)
\[ \omega_{p + 1} = \omega_p - \eta_p \circdot \odot L(\omega_p) \], где \(\odot\) --- поэлементное умножение двух векторов
* Метод покоординатного спуска
\[ f(x) \to \min_{x \in E_n} \]
Алгоритм:
- Выбираем вектор \(x_0 \in E_n\) \\
\(\forall i: \)
1. фиксируем значение всех перменных, кроме \(x_i\)
2. \(f(x_i) \to min\) любым методом одномерной оптимизации(золотое сечение наиболее популярный)
3. Проверка выполнения критерия останова:
   - \(\Vert x^{k + 1} - x^{k} \Vert \le \varepsilon_1\)
   - \(\Vert f(x^{k + 1}) - f(x^k) \Vert \le \varepsilon_2\)



