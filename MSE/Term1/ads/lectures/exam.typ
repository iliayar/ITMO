#import "../../../../other/typst/lecture_mse.typ": *

#show: doc => lecture(
  subj: [Алгоритмы и структуры данных],
  title: [Вопросы к экзамену],
  date: [23 января],
  number: 0,
  program: "ITMO MSE y2025",
  doc
)

// FIXME: For old typst compatibility
#let chevron = math.angle

#let theormin = [
  #h(-50pt - 7pt)#text(green)[`(a)`]#h(28pt + 7pt)
]

#let section(body) = [
  #h(16pt)#text(rgb(0,0,255), size: 16pt, weight: "bold")[#body]
]

= Вопросы
#section[Асимптотика]
== Хорошесть алгоритма: асимптотика; константа; память, кеш; простота идеи, реализации.
Как сравнивать разные алгоритмы:
1. Скорость работы
2. Память
3. Простота/понятность
4. Всегда работает и за работает конечное время
5. Детерминированность (запрещаем рандом)
6. Масштабирование (распараллеливание)
#fixme()
== #theormin Определения: $cal(O)(n)$, $Theta(n)$.
#definition()[
  $f in cal(O)(g)$ --- $exists N med forall n >= N med f(n) <= C dot g(n)$.

  Начиная с какого места $f <= C dot g$.
]
#definition()[
  $f = Theta(g)$ --- $exists C_1, C_2 >= 0 med C_1 dot g(n) <= f(n) <= C_2 dot g(n)$.
]

== Определения: $o(n)$, $Omega(n)$, $omega(n)$.
#definition()[
  $f = o(g)$, тогда
  $ forall C > 0 med exists N med forall n >= N med f(n) <= C dot g(n) $
]
#remark()[
  $f = o(g) <==> lim f / g = 0$
]

#definition()[
  $f = Omega(g) <==> exists C med exists N med forall n >= N med f(n) >= C dot g(n)$
]

#definition()[
  $f = omega(g) <==> forall C med exists N med forall n >= N med f(n) >= C dot g(n)$
]
== Упражнения на понимание: $cal(O)(Theta(cal(O)(f)))$, $o(cal(O)(Theta(f)))$, $Omega(Theta(cal(O)(f)))$.
#example()[
  - $f = attach(limits(o), b: <=)(attach(limits(Theta), b: =)(attach(limits(cal(O)), b: <=)(g)) ==> f = o(g)$
  - $f = attach(limits(Omega), b: >=)(attach(limits(Omega), b: >=)(attach(limits(Theta), b: =)(g))) ==> f = Omega(g)$
  - $f = attach(limits(Theta), b: =)(attach(limits(omega), b: >=)(attach(limits(omega), b: >=)(g))) ==> f = omega(g)$
]
== Упражнения на доказательства: $cal(O)(cal(O)(f)) = cal(O)(f)$, $cal(O)(C dot f) = cal(O)(f)$, $f + o(f) = Theta(f)$.
#example()[
$cal(O)(cal(O)(f)) = cal(O)(f) <==> forall g med g in cal(O)(cal(O)(f)) <==> g in cal(O)(f)$. н.у.о зафиксируем $g$:
- $(==>)$
  $ g in cal(O)(cal(O)(f)) ==> forall g' in cal(O)(f) med exists C_1 med exists N_1 med forall n >= N_1 med g <= C_1 dot g' ==> exists C_2 med exists N_2 med forall n >= N_2 med g' <= C_2 dot f $
  $ forall g' med exists C_1, C_2 med exists N_1, N_2 med forall n >= max(N_1, N_2) med g <= C_1 dot g' and g' <= C_2 dot f ==> g <= C_1 dot C_2 dot f ==> \
    ==> exists C := C_1 dot C_2 med exists N := max(N_1, N_2) med forall n >= N med g <= C dot f ==> g = cal(O)(f) $
- $(<==)$
  $ g in cal(O)(f) ==> exists C med exists N med forall n >= N med g <= C dot f ==> \
    exists C_1 := 1, C_2 := C med exists N_1 := 0, N_2 = N med forall n: n >= N_1 and n >= N_2 med  g <= C_1 dot C_2 dot f = C dot f ==> cal(O)(cal(O)(f)) $
  #fixme()
]
== Стандартные времена работы: полилог, полином, экспонента.
- Полином $cal(O)(n^2), cal(O)(n^3), cal(O)(sqrt(n)) = cal(O)(n^(1 / 2)), cal(O)("poly"(n))$
- Экспонента $cal(O)(2^n), cal(O)(2^sqrt(n)), cal(O)(2^("poly"(n)))$
- Полилогарифм $cal(O)(log n), cal(O)(log^2 n), cal(O)("poly"(log n))$
#remark()[
  Куда $n log n$. Можно зажать между двумя полиномами $n <= n log n <= n^2$, поэтому это полиномиальное время. Классы не пересекаются
]
== Время работы цикла `for` для чисел Фибоначчи. $Theta(n)$, $Theta(n^2)$.
```python
# f[N]
f[0] = 1;
f[1] = 1;
for i in range(2, N + 1):
  f[i] = f[i - 1] + f[i - 2]
```
1. На питоне:
  Число Фибоначи примерно $phi^n = (1 + sqrt(5)) / 2$. Чтобы хранить число нужен массив цифр цисла. Т.е. сложение двух числе за $cal(O)(n)$. Это честное сложение двух чисел. То есть код выше работает за $cal(O)(n^2)$.
2. Числа складываются не честно, поэтому $cal(O)(n)$.
== Модели вычислений: RAM, RAM-w, $log n <= w$, bitset, сложение массивов за $cal(O)(1)$.
#definition()[
  *RAM модель* --- все операции с числами выполняются за $cal(O)(1)$. Арифметические операции, чтение, вывод одного числа, _доступ к элементу массива_.
]

#definition()[
  *RAM-word модель* --- все операции с числами размера не больше машинного слова выполняются за $cal(O)(1)$.
]
#remark()[
  `for i = 1..n` работает за $cal(O)(n)$ только если $log n <= w$.
]
#remark()[В RAM $"P" = "NP"$]

#remark()[bitset. Храним несколько числе размера слова. Операции за $cal(O)(N / w)$]
== Асимптотика времени работы `for i for j in range(i)` и `for i for j in range(0,n,i)`. Оценка $sum 1 / i = Theta(log n)$.
#remark()[
  Как оценить $sum 1/i$:
  - $sum ~ integral$. ($sum_(i=0) <= integral <= sum_(i=1)$) $integral 1 / x d x = ln x$
  - $A dot f(n) <= sum <= B dot f(n)$.
    $ 1/2 dot log n = 1/2 + 1/4 + 1/4 + 1/8 + 1/8 + 1/8 + 1/8 + dots <= sum 1 / i <= 1 / 1 + 1/2 + 1/2 + 1/4 + 1/4 + 1/4 + dots = log n  $
]
#remark()[
  $sum N / i = N dot sum 1 / i$
]
#section[Элементарные структуры данных]
== Массив обыкновенный. Интерфейс.
- $"get"(i)$ $cal(O)(1)$
- $"set"(i, x)$ $cal(O)(1)$
== Односвязный список `(head, x, next)`. Интерфейс.
```cpp
struct Node {
  Node* next;
  int x;
};
Node* head;
```

- $#smallcaps[AddBegin]$ $cal(O)(1)$
- $#smallcaps[DelBegin]$ $cal(O)(1)$
- Итерация за $cal(O)(n)$
== Двусвязный список. Интерфейс.
```cpp
struct Node {
  Node* next, prev;
  int x;
};
Node* head, tail;
```
== Вектор. Аллокация с удвоением. Интерфейс.
Массив + #smallcaps[AddEnd].
#remark()[
  Среднее время работы:
  $ (sum overbrace(1 + 1 + 1 + dots + 1, n / 2) + n) / (n / 2) < 3 = Theta(1) $
]
== Циклический массив. Аллокация с удвоением.
- #smallcaps[AddBegin] `a[--L] = x` по модулю $n$
- #smallcaps[AddEnd] `a[R++] = x` по модулю $n$
- Когда `size = (R - L + n) % n` становится $0$, реаллоцируем
== #theormin Интерфейсы стек/очередь/дек.
- Стек: `push`, `pop` LIFO
- Очередь: `push`, `pop` FIFO
- Дек: `push_back`, `push_fron`, `pop_back`, `pop_front`
== Сравнение реализаций стека/очереди/дека через циклический массив и список.
Реализации:
#table(
  columns: 2,
  stroke: none,
  [Список], table.vline(), [Массив],
  table.hline(),
  [#text(green)[+] $cal(O)(1)$], [#text(red)[-] $tilde(cal(O))(1)$],
  [#text(red)[-] Кэш не работет], [#text(green)[+] работает],
  [#text(red)[-] Требует больше памяти], [#text(green)[+] меньше],
  [#text(red)[-] Нет доступа к $i$-ому элементу], [#text(green)[+] есть],
  [#text(green)[+] Вставка в середину за $cal(O)(1)$], [#text(red)[-] $cal(O)(n)$]
)
#remark()[
  $tilde(cal(O))$ --- в среднем
]
== Амортизационный анализ. Метод потенциалов, анализ вектора.
#definition()[
  $ frac(sum_(i = 1)^n #text[time]_i, n) = #text[average] $
]

#definition()[
  Потенциал: $forall phi_i in RR$.
  $ a_i = #text[time]_i + Delta phi quad Delta phi = phi_(i + 1) - phi_i $
  где $a_i$ -- *амортизированное время*
]
#remark()[
  $ sum_(i = 1)^m a_i = sum_(i = 1)^m #text[time]_i + (phi_m - phi_0) $
  $ frac(sum_(i = 1)^m a_i, m) = frac(sum_(i = 1)^m #text[time]_i, m) + frac(phi_m - phi_0, m) $
]
#theorem()[
  $ forall phi quad frac(sum_(i = 1)^m #text[time]_i, m) <= max a_i + frac(phi_0 - phi_m, m) $
]
#proof()[
  Очевидно, учитывая что $frac(Sigma, m) <= max$
]

#remark()[
  $phi_0 = 0 med forall i : phi_i >= 0 ==> frac(phi_0 - phi_m, m) <= 0 ==>$
  $ frac(sum #text[time]_i, m) <= max a_i $
]

#example()[
  Рассмотрим вектор. $phi = - #text[capacity]$. Рассмотрим два случая $a_i = #text[time]_i + Delta phi$:
  - Не реаллоицировали: $a_i = 1 + 0 = 1$
  - Реаллоцировали: $a_i = n + (-n) = 0$
  $ a_i = Theta(1) ==> frac(sum #text[time]_i, m) = Theta(1) + frac(2m, m) = cal(O)(1) $
]
== Очередь на двух стеках. Очередь с минимумом. Анализ через потенциалы.
1. Стек с минимумом = $"Stack" + "Min"$. Дополнительно храним стек с минимумом из последнего элемента на этом стеке и новым элементом
2. Очередь с минимумом. Реализуется на 2 стеках с минимумами ($A, B$).
   #pseudocode-list("Доступные операции")[
     - #smallcaps[Push] ($x$)  
       + $A."Push"(x)$  
     -
     - #smallcaps[Pop] ($x$)  
       + *if* $B."Empty"()$
         + *while* $not A."Empty"()$ --- $cal(O)(|A|)$
           + $B."Push"(A."Pop"())$
       + *return* $B."Pop"()$
     -
     - #smallcaps[Min] ()
       + *return* $min(A."Min"(), B."Min"())$
   ]

#proof()[
  $phi = |A|$
  - Min :: $a_i = 1 + 0$
  - Push :: $a_i = 1 + 1$
  - Pop :: $a_i = cases(1 + 0, |A| - |A| = 0)$
  $ frac(sum #text[time]_i, m) <= max a_i = Theta(1) $
]
== _Дек с минимумом. Анализ через потенциалы._
Два стека с минимумами:
- `push_back` --- добавляем в $A$
- `push_front` --- добавляем в $B$
- `pop_front` --- снимаем с $B$, если пусто --- перемещаем половину элементов из $A$ в $B$, переворачиваем стеки там где нужно. Кажется $T <= 2n$.
- `pop_back` --- аналогично
В качестве потенциала выберем $phi = 4 dot |A - B|$.
- Если не перекладываем: $a_i = 1 - 4 = -3$ или $a_i = 1 + 4 = 5$. В обоих случаях $cal(O)(1)$.
- Если перекладываем: $a_i = 2n + (4 dot n / 2 - 4 dot n) = 0 = cal(O)(1)$
Т.к. $phi_0 = 0, phi_i >= 0$, то $T <= max a_i = cal(O)(1)$. 
#fixme()
== _Два указателя: отрезок фиксированной суммы; отрезок min длины с k различными._
- Отрезок суммы $S$. Двигаем правую границу пока сумма меньше $S$. Двигаем левую пока больше $S$. Повторяем.
- Отерок $min$ длины с $k$ различными. Поддерживаем мапу количества элементов на отрезке. Если меньше $k$ разлиных двигаем правую границе. Если больше или равно двигаем левую. Повторяем
#fixme()

#section[Метод разделяй и властвуй]
== #theormin MergeSort. Собственно сортировка.
Очев.
== MergeSort. _Подсчёт числа инверсий в массиве._
При мерже если встрелил $L_i > R_j$, то получается что $dots > L_(i + 1) > L_i > R_j$. Т.е. образуется $|L| - i$ инверсий. Суммируем это количество инверсий при мерже и суммируем из каждой половины. #fixme()
== Рекуррентные соотношения, дерево рекурсии, док-во времени работы MergeSort $cal(O)(n log n)$.
Время работы этой сортировки:
$ T(n) = 2 dot T (frac(n, 2)) + n $
Посмотрим на дерево рекурсии: каждый раз размер задачи уменьшается в $2$ раза. Всего "уровней" $log n$, на каждом уровне ровно $2^i dot n / 2^i = n$ операций. $T(n) = n log n$. 

#remark()[
  Чтобы не было проблем с округлением можно "зажать" $n$ между степенями двойки $2^(k - 1) <= n <= 2^k$ в данном случае.
]
#remark()[
  Можно сказать что один рекурсивный вызов от $x$ а другой за $n - x$, так тоже можно избавиться от проблем с округлением. 
]
== Рекуррентные соотношения: $T (n) = T ( n/2 )+n, T (n) = 3T ( n/2 )+n$, обобщение до Мастер-Теоремы.
#example()[
  $ T(n) = 2 dot T (frac(n, 2)) + n $
  Количество слоев также $log n$, но на каждом слое теперь не $n$, а убывающая геометрическая прогрессия:
  $ n (1 + frac(1, 2) + frac(1, 4) + dots ) = Theta(1) $
]

#remark()[
  $ 1 + alpha + alpha^2 + dots + alpha^k = frac(alpha^(k + 1) - 1, alpha - 1) = Theta(alpha^k) $
]

#example()[
  $ T(n) = 3 dot T (frac(n, 2)) + n $
  Слоев столько же, но каждом слое теперь $frac(3, 2) n$
  $ n (1 + frac(3,2) + (frac(3,2))^2 + dots) = Theta(n dot (frac(3, 2))^(log n)) = Theta(3^(log n)) = Theta(n^(log 3)) $
]

#theorem("Мастер теорема")[
  $T(n) = a dot T(frac(n, b)) + n^c$, где $a > 0, b > 1, c >= 0$. Тогда:
  1. $a = b^c ==> T(n) = Theta(n^c dot log n)$
  2. $a < b^c ==> T(n) = Theta(n^c)$
  3. $a > b^c ==> T(n) = Theta(n^(log_b a))$
]
#proof()[
  $ T(n) = n^c dot (1 + (frac(a, b^c)) + (frac(a, b^c))^2 + dots) = n^c sum_(i=0)^(log n) (a / b^c)^i $
  1. $a / b^c = 1 ==> T(n) = Theta(n^c log n)$
  2. $a / b^c < 1 ==> T(n) = Theta(n^c)$, т.к. $sum^infinity alpha^i = "const"$, т.к. при $alpha < 1$ ряд сходится.
  3. $a/ b^c > 1$. Сумма равна $1 + alpha + alpha^2 + dots = (alpha^(k + 1) - 1) / alpha ~ a^k $. Значит :
  $ T(n) ~ n^c (a / b^c)^(log_b n) = n^c (a^(log_b n) / undershell((b^(c dot log_b n)), n^c)) = n^(log_b a) $
]
== #theormin Двоичный поиск: каноничная 0011-версия.
Поиск по массиву из $0$ и $1$.
#pseudocode-list()[
  - $#smallcaps[BinSearch] (A)$
    + $l <- -1$
    + $r = n$
    + *while* $r - l > 1$
      + $m <- (l + r) / 2$
      + *if* $A_m$
        + $r <- m$
      + *else*
        + $l <- m$
    + *return* $r$
]
== Двоичный поиск: сведение задач `lowerBound`, `sqrt(x)` к 0011-версии.
$P thick a_i |-> {0, 1}$ --- монотонный предикат. $"argmin"_i P(a_i) = 1$. Вместоп $A_m$ $P(A_m)$.
#example()[
  - `lower_bound` Первый $a_i >= x$ --- $P(a_i) = a_i >= x$
  - `upper_bound` Первый $a_i > x$ --- $P(a_i) = a_i > x$
]

Бинпоиск по ответу. $P: NN -> {0, 1}$ или $RR -> {0, 1}$. Нужно найти что-то на $[x_min, x_max)$;
#pseudocode-list()[
  - $#smallcaps[BinSearch] ()$
  + $l <- x_min$
  + $r = x_max$
  + *while* $r - l > 1$
    + $m <- (l + r) / 2$
    + *if* $P(m)$
      + $r <- m$
    + *else*
      + $l <- m$
  + *return* $r$
]
#example()[
  $sqrt(x), 0 <= x <= C$. $x_min = 0$, $x_max = C$, $P(m) = m^2 > x$. Если нужно с точностью до $k$ знаков после запятой, то дополнительно $k log_2 10$ операций.
]
== _Двоичный поиск: сколько раз число встречается на отрезке? за $cal(O)(log n)$._
Сделайте предподсчет за $cal(O)(n log n)$, чтобы за $cal(O)(log n)$ online отвечать на запрос “сколько раз число $x$ встречается на отрезке $[l dots r]$?
#solution()[
  Строим массив пар $chevron.l a_i, i chevron.r$, сортируем. бин-поиск по парам запросов $chevron.l x, l chevron.r, chevron.l x, r chevron.r$.
]
== _Двоичный поиск по ответу: отправка коров в стойла так, чтобы $min "dist" -> max$._
Есть $m$ стойл с координатами $x_1, dots, x_m$ и $n$ коров. Расставить коров по стойлам (не более одной в стойло) так, чтобы минимальное расстояние между коровами было максимально. $cal(O)(m(log m + log x_max))$.

#solution()[
  Бин. поиск по предикату $P(l) =$ можно расставить $n$ коров с расстоянием между соседними не меньше $l$. $P(l)$ монотонно убывает --- при маленьких $l$ выполняется, при больших нет. Бинарным поиском найдем такое максимальное $l$ что $P(l)$ что и будет ответом.

  Чтобы посчитать $P(l)$ будем идти по отсортированному массиву координат и жадно ставить коров, если расстояние до предыдущей $>= l$ (для первой расстояние до предыдущей $infinity$). Почему расставим оптимально: пусть не смогли расставить, но существует решение. Посмотрим на первую точку, которая не совпадает: она не может стоять левее потому что тогда расстояние до предыдущей будет $< l$, т.к. если все предыдущие совпадают, противоречие. Если она стоит правее, то подвинем нашу точку также правее и "по индукции" получим такое-же или худшее решение, противоречие. Значит наша реализация $P(l)$ оптимальна.

  Общее время складывается из сортировки массива координат размера $m$ и бинарного поиска по значениям $[0; x_max]$ по предикату который проходится по массиву координат размера $m$.
  $ T(n) = cal(O)(m log m) + cal(O)(log x_max dot m) = cal(O)(m(log m + log x_max)) $
]
== #theormin Умножение многочленов: $cal(O)(n^2)$.
$A, B$ массивы коэффициентов. $P_A = A_0 x^0 + A_1 x^1 + dots + A_n x^n$
#pseudocode-list()[
  + $C <- {0, dots, 0}$
  + *for* $i <- 0, dots, |A| - 1$
    + *for* $j <- 0, dots, |B| - 1$
      + $C_(i + j) <- C_(i + j) + A_i dot B_j$
]
== Умножение многочленов: рекурсия за $cal(O)(n^2)$, Карацуба за $cal(O)(n^(log 3))$.
Умножаем многочлены $A(x) dot B(x)$, где $A$, $B$ --- массивы коэффициентов. н.у.о $n = 2^k$.

Разделим массив $A$ на два $A_1$ и $A_2$. Тогда $A(x) = A_1(x) + x^(n / 2) A_2(x)$

$ A dot B = (A_1 + x^(n / 2) A_2) dot (B_1 + x^(n / 2) B_2) = A_1 dot B_1 + x^(n / 2) (A_1 dot B_2 + A_2 dot B_1) + x^n A_2 dot B_2 $
Сложения и сдвиги массивов умеем за линию. Тогда $T(n) = 4 T(n / 2) + n = O(n^2)$. Заметим что
$ A_1 B_2 + B_2 A_1 = (A_1 + A_2) (B_1 + B_2) - A_1 B_1 - A_2 B_2 $
Получается нужно посчитать только $A_1 B_1$, $A_2 B_2, (A_1 + A_2) (B_1 + B_2)$. Тогда $T(n) = 3 T(n / 2) + n = O(n^(log_2 3))$
== Умножение чисел: сведение к многочленам, переносы.
Будем хранить число $A$ как многочлен
$ A = A_0 dot B^0 + dots + A_n dot B^n $

Можем перемножить числа $A$ и $B$ Карацубой. Получим массив коэффициентов $C$. Но коэффициенту могут быть большими.
#example()[
  $A = 15, B = 15$. После перемножения $C = 25 + 10 x + x^2$. Нужно перекинуть больше коэффициенты на разряд больше $C = 5 + 2 x + 2 x^2$.
]
#section[Сортировки и куча]
== Нижняя оценка на сортировки: $Omega(n log n)$ сравнений.
#statement()[
  $forall$ детерминированная сортировка на $<$ $Omega(n log n)$
]
#proof()[
  Пусть делаем $<= k$ сравнений. Корректная сортировка сортирует как минимум все перестановки, которых $n!$.
  Докажем что $2^k >= n!$: Сравнение возвращает 1 бит информации (меньше или больше или равно).
  Пусть сортировка всегда делает $k$ операций. В результате получилось $k$ результатов сравнения ($k$ бит).
  Пусть $2^k < n!$, тогда $exists p != q$, ($p, q$ -- перестановки), т.е. у них одинаковые результаты сравнения, значит алгоритм делал одни и те же действия, значит одна из не будет отсортирована. Значит $2^k >= n! => k => log n!$.
]

#proof()[
  Рассмотрим дерево выполнения алгоритма. Каждый узел это сравнение $x_i < x_j$. У каждого узла два исхода. В листах алгоритм сделал достаточное количество сравнений чтобы отсортировать массив. Высота этого дереве --- худшее время работы $h = T(n)$. Если запустим алгоритм на разных перестановках, то должны придти в разные листы. Т.к. если придем в один и тот же лист, то значит что отсортируем одну из перестановок неправильно. Значит
  $ #text[количество листьев] >= n! $

  Количество листьев не может быть больше $2^h = 2^(T(n))$. Значит
  $ T(n) >= log n! ~ n log n $
]

#remark()[
  $ (n / 2)^(n / 2) <= n! <= n^n ==> n / 2 log n / 2 <= log n ! <= n log n ==> log n ! = Theta(n log n) $
]
== _Нижняя оценка на время сортировки, корректной для $1 / 100^n $ доли перестановок: $Omega(n log n)$._
#solution()[
  Аналогично доказательству нижней границы сравнений любой сортировки
  $ 
    2^k >= frac(n!, 100^n) \
    k >= log frac(n!, 100^n) \
    k >= log n! - n dot underbrace(log 100, ~6.6) \
    k = Theta(n log n) - n dot log 100
  $
]
== InsertionSort, оценка времени работы $cal(O)(n + #[число инверсий])$.
Префикс отсортирован, свапаем очередной элемент с предыдущим пока он не попадет на свое место.
#definition()[
  Инверия $i, j : i < j and a_i > a_j$.
]
Заметим что свап срабатывает только для инверсий, значит время работы $cal(O)(n + I)$, где $I$ --- число инверсий
#remark()[
  Можно не свапать а искать место для вставки бин-поиском.
]
== SelectionSort, преимущество над всеми другими сортировками.
Префикс осторирован и элементы в этом префиксе меньше чем все остальные. Находим минимальный из оставшимся и добавляем в конец префикса. *Делает мало свопов* $cal(O)(n)$.
#example()[
  Нестабильный ${5_0, 5_1, 2}$. Поменяем на первом шаге ${2, 5_1, 5_0}$.
]
== #theormin Куча: интерфейс (extractMin, add). Сортировка кучей.
$p$ --- массив предков. $h_i >= h_(p_i)$.
- $#smallcaps[Insert] (x)$
- $#smallcaps[GetMin] ()$
- $#smallcaps[ExtractMin] ()$

Добавим все элементы из массива в кучу, а потом достанем. #fixme()
== Бинарная куча: хранение, siftUp, siftDown, add, extractMin.
- $"root" = 0$
- $"left"(i) = 2i + 1$
- $"right"(i) = 2i + 2$
- $"parent"(i) = floor((i - 1) / 2)$

- #smallcaps[SiftUp] $cal(O)(log n)$. Если уменьшили значение элемента, то могли сломать свойство кучи, фиксим вызывая #smallcaps[SiftUp].
- Аналогично если увеличили, вызываем #smallcaps[SiftDown].
== Бинарная куча: построение за $cal(O)(n)$, оценка времени.
Построение за $O(n)$ -- делаем `SiftDown(i)` для $i colon.eq n dots 1$ (с конца). Идем по поддеревьям размера $1, 2, dots$ восстанавливая свойство кучи для поддеревьев вызывая `SiftDown`. В дереве листьев примерно $n / 2$. На листьях можно не вызывать, значит можно начинать с $n / 2$:
$ T(n) <= sum_(k = 1)^h k 2^(h - k) = 2^h sum_(k = 1)^h k / 2^k ~ n sum_(k = 1)^h k / 2^k $
$ sum k / 2^k = & 1/2 + 2 dot 1/4 + 3 dot 1/8 + 4 dot 1/16 + dots = 2 $
$               & 1/2 + 1/4 + 1/8 + 1/16 + dots & = 1 \
                & #hide[$1/2 +$] 1/4 + 1/8 + 1/16 + dots & = 1/2 \
                & #hide[$1/2 + 1/4 +$] 1/8 + 1/16 + dots & = 1/4  \
                & #hide[$1/2 + 1/4 + 1/8 + 1/16 + dots$] & eq 2
$
где $k$ это слой дерева (поддеревья высоты $k + 1$).
$ T(n) ~ n dot 2 = Theta(n) $
== _Бинарная куча: поддержка медианы массива с помощью кучи._
Две кучи: по минимуму и по максимуму. Поддерживаем одинаковый размер этих двух куч. #fixme()
== Inplace версия сортировки кучей (без доппамяти).
- Построим кучу за $Theta(n)$.
- Будем делать #smallcaps[ExtractMax], который будет перемещать очередной максимум в конец кучи. Когда достанем все из кучи останется отсортированный массив.

Не требуем дополнительной памяти
== Куча: decreaseKey, deleteAny, обратные ссылки.
Храним массив позиций элементов в куче по их значениям $"pos"$, такой что $"pos"_(h_j) = j$ и $h_("pos"_i) = i$. Можем уменьшать ключ и удалять элементы за SiftUp/SiftDown.
== C++: `prioirity_queue`, `set`; python: `heapq.heappush`.
Очев. #fixme()


#section[Сортировки-2]
== Понятие вероятностного алгоритма. Отличие понятий "амортизированно" и "в среднем".
_Задача_: Нужно найти элемент в массиве, который встречается больше половины раз.

_Решение_: Выбрать случайный элемент и проверить $x = a[i = #text[Random] med 0 dots n-1]$.

#definition()[
  $#text[Time] = EE_(#text[Random])$
]
Посчитаем время: $EE = n + frac(1, 2) dot 0 + frac(1, 2) dot E$ --- всегда проверяем за $n$, с вероятностью $frac(1, 2)$ ничего не делаем т.к. нашли, с вероятностью $frac(1, 2)$ не находим и повторяем. $E = 2n$

В среднем:
- $T(n) = (sum t_i) / m = cal(O)(t(n))$
- $EE_"Random" T(n) = cal(O)(t(n))$

#remark()[
  - Вероятностные могут работать долго, но с маленькой вероятностью
  - Амортизированные точно иногда могут работать долго
]

== #theormin QuickSort. Простая реализация с доппамятью.
#pseudocode-list()[
  - $#smallcaps[QSort]""(A)$
    + *if* $|A| <= 1$
      + *return* $A$
    + $p = "pivot"(A)$
    + $A_1, A_2, A_3 = "partition"(A, A_p)$
    + *return* $#smallcaps[QSort]""(A_3) + A_2 + #smallcaps[QSort]""(A_3)$
]
Время будет зависить от того как выбарли $p$
  $ T(n) = n + T(p) + T(n - p) $
- $T(n) = n + T(C) + T(n - C) ==> Theta(n^2)$
- $T(n) = n + T(alpha n) + T((1 - alpha)n) ==> Theta(n log n)$ Если $alpha = 1 / 2$
== QuickSort. Доказательство через оценку числа сравнений $sum_(i,j) 2 / (j−i+1)$.
#proof()[
  Пусть $chi_(i, j) = cases(1"," #text[сравнивали $x_i$ и $x_j$], 0"," #text[не сравнивали])$. Тогда $T(n) = sum_(i < j) chi_(i, j)$.
  $ PP(chi_(i, j) = 1) = 2 / (j - i + 1) $
  Т.к. "удачные" случаи когда выбрали $i$ или $j$ pivot'ом. Если выбрали элемент между этими двумя в отсортированном массиве то никогда их не сравним, если выбрали левее и правее то они попадут в одну партицию --- всего исходов $j - i + 1$ --- длина этого отсортированного промежутка.
  $ T(n) = sum_(i < j) 2 / (j - i + 1) = 2 sum_(i = 0)^(n - 1) sum_(j = i + 1)^(n - 1) 1 / (j - i + 1) = 2 sum_(i = 0)^(n - 1) sum_(k = 0)^(n - i) 1 / k = \
    = 2 sum_(i = 0)^(n - 1) (ln (n - i + 1) + cal(O)(1)) = cal(O)(n) + 2 sum_(i = 1)^(n) ln i = cal(O)(n) + 2 ln n! = \
    = 2 (n ln n - n + cal(O)(log n)) + cal(O)(n) = 2 n ln n + cal(O)(n)
  $
]
== QuickSort. Доказательство заменой суммы на интеграл.
#remark()[
  Можем заменять сумму на интеграл. Перейдем от интеграла к сумме, разбив на прямоугольники, получим $sum_(i = 0)^infinity <= integral$. Сдвинем сумму влево на один прямоугольник, тогда получим $sum_(i = 1)^infinity >= integral$. Получается что
  $ sum_(i = 0)^infinity <= integral <= x_0 + sum_(i = 1)^infinity ==> integral = Theta(sum) $
]
#fixme()
== #theormin QuickSort. Inplace partition. Алгоритм.
#pseudocode-list()[
  - $#smallcaps[Partition]""(A, l, r, x)$
    + $i <- l$
    + $j <- r$
    + *while* $i <= j$
      + *while* $A_i < x$
        + $i <- i + 1$
      + *while* $A_j > x$
        + $j <- j - 1$
      + *if* $i <= j$
        + $"swap"(A_i, A_j)$
        + $i <- i + 1$
        + $j <- j - 1$
    + *return* $chevron.l i, j chevron.r$
]
== QuickSort. Inplace partition. Оценка времени работы.
Очев. $cal(O)(n)$ #fixme()
== QuickSort. Элиминация хвостовой рекурсии и оценка $cal(O)(log n)$ на доппамять.
#pseudocode-list()[
  - $#smallcaps[F]""(x)$
    + *if* `<cond>`
      + *return*
    + `<statements>`
    + #smallcaps[F]$("expr"(x))$
]
Можем избавиться от рекурсии
#pseudocode-list()[
  - $#smallcaps[F]""(x)$
    + *while* $!$ `<cond>`
      + `<statements>`
      + $x <- "expr"(x)$
]
Можем избавиться от второго рекурсивного вызова рекурсии в QSort и вызваться всегда только от меньшей части. Тогда глубина рекурсии всегда будет $cal(O)(log n)$.
== QuickSort: IntroSort.
Хорошо работает на практике:
#pseudocode-list()[
  - $#smallcaps[IntroSort]""(A, "depth")$
    + *if* $|A| <= 16$
      + $#smallcaps("InsertionSort")""(A)$
      + *return*
    + *if* $"depth" >= 2 log_2 n$
      + $#smallcaps[HeapSort]""(A)$
      + *return*
    + как #smallcaps[QuickSort]
]
== $k$-ая статистика: heap, $cal(O)(n + k log n)$.
Построить кучу на массиве за $cal(O)(n)$, достать $k$ элементов за суммарно $cal(O)(k log n)$. #fixme()
== $k$-ая статистика: одноветочный QuickSort за $cal(O)(n)$, оценка времени работы.
Задача: $k$-ый элемент в отсортированном массиве
- Одноветочный Quicksort. Сортируем тольку ту половину, где $k$-тый элемент.
  $ 
    EE T(n) <= n + frac(1, 2)T(frac(3, 4) n) + frac(1, 2)T(n) \
    EE T(n) <= 2n + T(frac(3, 4)n) = Theta(n) \
    2n (1 + frac(3, 4) + (frac(3, 4))^2 + dots)
  $
== $k$-ая статистика: детерминированная с делением на кусочки длины 5.
Поделим массив на куски длины $5$. Найдем в каждом медиану. Среди этих медиан найдем медиану рекурсинвным вызовом. Заметим что этот медианнай элемент $m$: $m >= 3 dot n / 10$ и $m <= 3 dot n / 10$. Получается что в отсортированном массиве эта медиана будет в промежутке $3/10 n$ и $7 / 10 n$. Если возьмем эту медиану в качестве pivot'а для QuickSelect'а:
  $ T(n) = n + T(n / 5) + T( 7/10 n ) = cal(O)(n) $
== _$k$-ая статистика: детерминированная с делением на кусочки длины 3 и 7._
#solution()[
  Решим для групп размером $2k + 1$. Тогда размер группы больше и меньше будет по $n dot (k + 1) / (4k + 2)$. Итоговое время:
  $ T(n) = Theta(n + n / (2k + 1) dot (2k + 1)^2) + T(n dot 1 / (2k + 1)) + T(n dot (1 - (k + 1) / (4k + 2))) $
  Нужно чтобы $1 / (2k + 1) + (3k + 1) / (4k + 2) < 1$.
  - Для $k = 1$: $1 / 3 + 4 / 6 = 1 lt.not 1$ *Не ок*
  - Для $k = 2$: $1 / 5 + 7 / 10 = 9/10 < 1$ *OK*
  - Для $k = 3$: $1 / 7 + 10 / 14 = 6/7 < 1$ *ОК*
]
== Inplace алгоритмы: reverse, rotate, swap-половинок-массива.
1. Reverse --- `swap(a[i], a[n - i - 1])`
2. Rotate:
  $C = A B$ --- `Reverse(A)`, `Reverse(B)` --- $A^R B^R$ --- `Reverse(C)` --- $B A$

  Можно делать swap соседних блоков в массиве
== _Inplace stable partition за $cal(O)(n log n)$._
- Разделим массив на две части, вызовемся рекурсивно. Алгоритм возвращает индексы блоков $< x$, $= x$, $> x$
- $limits(A_1)_(<x) limits(A_2)_(=x) limits(A_3)_(>x) limits(B_1)_(<x) limits(B_2)_(=x) limits(B_3)_(>x)$
- $A_1 B_1 A_2 B_2 A_3 B_3$

#section[Сортировки быстрее $n log n$]
== #theormin CountSort, простая версия.
Считаем все элементы одинаковыми
#pseudocode-list()[
  + *for* $a in A$
    + $"count"_(a."key") <- "count"_(a."key") + 1$
  + $j <- 0$
  + *for* $i <- 0 dots m - 1$
    + *for* $1 dots "count"_i$
      + $"res"_j <- i$
      + $j <- j + 1$
]
#fixme()
== CountSort для сортировка пар $chevron.l #[key], #[object] chevron.r$. Стабильность.
- $a_i = chevron.l "key", "value" chevron.r$
- $"key" in [0, m) inter ZZ$
#pseudocode-list()[
  + *for* $a in A$
    + $"count"_(a."key") <- "count"_(a."key") + 1$
  + $"pos" = {0, dots, 0}$
  - Считаем начала блоков для каждого значения ключа
  + *for* $i <- 1 dots m - 1$
    + $"pos"_i <- "pos"_(i - 1) + "count"_(i - 1)$
  + *for* $a in A$
    + $"res"_("pos"_(a."key")) = a$
    + $"pos"_(a."key") <- "pos"_(a."key") + 1$
]
#remark()[
  $Theta(n + m)$. Стабильная
]
== #theormin Частичные (префиксные) суммы.
$"pos"$ выше это массив частичных сумм $"count"$. #fixme()
== _Структура для $n$ чисел от $1$ до $k$ за $chevron.l cal(O)(n + k), cal(O)(1) chevron.r$ говорящая "сколько чисел от a до b"?_
Посчитаем массив с количествами чисел за $cal(O)(n)$, посчитаем частичные суммы на нем за $cal(O)(k)$. Можем находить количество за $cal(O)(1)$. #fixme()
== RadixSort. Реализация за $n log_n C$.
Сортируем $chevron.l a_1, a_2, dots, a_k chevron.r$. Сортируем, начиная с $a_k$. $cal(O)(k dot (n + m))$.

Как сортировать числа с помощью этого: Запишем число в системе счисления $"BASE"$ как кортеж. Получим $cal(O)(log_("BASE") m dot (n + "BASE"))$. Если $"BASE" = 2^k$, то $cal(O)((log m) / k dot (n + 2^k))$. Если $k ~ log n$, то $cal(O)(n log_n m)$.
#statement()[
  Выгодно брать $"BASE" approx n$. Чтобы эффективно получать $i$-ую цифру надо брать $"BASE" = 2^L$.
]

== BucketSort для равномерно распределённых вещественных чисел.
Разобъем числовую прямую на $n$ кусочков длины $frac("Max" - "Min" + 1, n)$. $x$ попадет в бакет $floor.l frac(x - "Min", "Max" - "Min" + 1) dot n floor.r$. Отсортируем каждый бакет:
- Insertion
- Quick sort
- Bucket sort

#theorem()[
  Если данные равномерно распределены то $EE T = Theta(n)$.
]
#proof()[
  $T = sum_b "size"_b^2 = sum_(i, j) ["ind"_i = "ind"_j]$, где $b$ - бакет, ind -- номер бакета.
  $ EE T = sum_(i, j) PP ["ind"_i = "ind"_j] = $
  , где $PP ["ind"_i = "ind"_j] = cases(i = j"," & 1, i != j"," & frac(1, n))$
  $ = n (n - 1) dot frac(1, n) + n = 2n - 1 $
]
== Несколько CountSort-ов в одном: сортировка массивов $A_1, A_2, dots, A_k$ над алфавитом $m$ за $O(m + sum_i |A_i|)$.
Пусть хотим отсортировать несколько массивов $A_1, A_2, dots, A_s$, все элементы которых не больше $m$. Radix sort сработает за $sum (|A_i| + m)$. Как отсортировать за $sum |A_i| + m$: Сделаем большой элемент пар из элементов массива вместе с индексом массива $chevron.l x, i chevron.r$. Отсортируем их за $sum |A_i| + m$. Пройдемся по сортированным парам с сложим в соответсвующие места.
== Рекурсивный перебор: рюкзак со стоимостями, решение за $O(2^n)$.
$sum_(i in S) w_i <= W$, $sum_(i in S) c_i -> max$
#pseudocode-list()[
  - $#smallcaps[Go]""(i, "sumW", "sumC")$
    + *if* $i = n$
      + *if* $"sumW" <= W$
        + $"ans" = max("ans", "sumC")$
        + *return*
    + $#smallcaps[Go]""(i + 1, "sumW", "sumC")$
    + $#smallcaps[Go]""(i + 1, "sumW" + w_i, "sumC" + c_i)$
]
== Рекурсивный перебор: перебор перестановок за $O(n!)$.
#pseudocode-list()[
  - $p <- {1, dots, n}$ --- массив перестановки
  - $"used" <- {"false", dots}$ --- какие элементы уже взяли
  - $#smallcaps[Go]""(i, "sumW", "sumC")$
    + *if* $i = n$
      + Полезные действия с перестановкой $p$
    + *for* $p_i <- 1 dots n$
      + *if* $"used"_(p_i)$
        + *continue*
      + $"used"_(p_i) <- 1$
      + $#smallcaps[Go]""(i + 1)$
      + $"used"_(p_i) <- 0$
]
Это работает за $cal(O)(n! dot n)$. Можем хранить в $p$ все индексы, тогда перебор индексов котоыре еще не в префиксе перестановки можно делать по элементам вне префикса
#pseudocode-list()[
  - $p <- {1, dots, n}$ --- массив перестановки
  - $"used" <- {"false", dots}$ --- какие элементы уже взяли
  - $#smallcaps[Go]""(i, "sumW", "sumC")$
    + *if* $i = n$
      + Полезные действия с перестановкой $p$
    + *for* $j <- i dots n - 1$
      + $"swap"(p_i, p_j)$
      + $#smallcaps[Go]""(i + 1)$
      + $"swap"(p_i, p_j)$
]
Это уже работает за $cal(O)(n!)$.
== VEB. Куча, которая умеет всё за $log log C$: общее устройство, откуда $log log$ ?
Куча за $O(log log n)$.
```cpp
struct VEB {
  int U;
  int min, max;
  VEB buckets[]; // #$sqrt(U)$ бакетов
  VEB nonempty; // индекс непустых
};
```

Разбиваем диапазон чисел $[0; C]$ делим на кусочки по $sqrt{C}$. В каждом кусочке храним V.E.B $"buckets"$. Глубина $log log C$. Заведем еще V.E.B который хранит номера не пустых кусков $"nonempty"$. Храним также размер $"size"$ и максимальный и минимальный элемент $"min", "max"$.
#remark()[
  Что такое $O(log log n)$: Извлекаем корни, т.к. длина числа $sqrt(C) ~ frac(log C, 2)$. Уменьшаем *длину* числа в два раза на каждом уровне. Т.е. высота дерева это $log(#[длина числа])$. А длина числа это $log n$.
]
== VEB. Куча, которая умеет всё за $log log C$: add, extractMin, lowerBound.

#pseudocode-list()[
  - #smallcaps[ExtractMin] ()
    + $i <- floor.l "min" / sqrt(C) floor.r$
    - $"buckets"[i]$ --- Куча в которой минимум
    + *if* $"buckets"[i]."size" = 1$
      + $"nonempty"."ExtractMin"()$
    + *if* $"buckets"[i]."size" >= 1$
      +  $"buckets"[i]."ExtractMin"()$
    + $"size" <- "size" - 1$
    + $"min" <- "buckets"["nonempty"."min"]."min"$
]

#pseudocode-list()[
  - #smallcaps[Add] ($x$)
    + $i <- floor.l x / sqrt(C) floor.r$ 
    + *if* $"buckets"[i]."size" = 0$
      + $"nonempty"."add"(i)$
      + $"buckets"[i]."size" <- 1$
      + $"buckets"[i]."min" <- x$
      + $"buckets"[i]."max" <- x$
    + *else*
      + $"buckets"[i]."Add"(x)$
    + $"min" <- min ("min", x)$
    + $"max" <- max ("max", x)$
]

Можно искать предшественника (successor, lowerBound). Спускаемся до дерева где в бакете ровно один или 0 искомых элементов. Берем максимум и предыдущего бакета $fixme()$.

#remark()[
  $cal(O)(U)$ памяти. Если хранить не массив а хеш мапу, т.е. не хранить пустые VEB'ы, то $cal(O)(n log log U)$.
]

#section[Динамика-1]
== #theormin Рекуррентные соотношения: $#[fib]_n, C_(n,k)$ (биномиальные), $f[L, R] = min_M f[L, M ] dot f [M, R]$. Решение всех данных рекуррентных соотношений динамикой.
#pseudocode-list()[
  - $#smallcaps[Fib]""(n)$
    + *if* $n < 2$
      + *return* $n$
    + *return* $#smallcaps[Fib]""(n - 1) + #smallcaps[Fib]""(n - 2)$
]
#pseudocode-list()[
  + $"dp" <- {0, 1, -1, -1, dots}$
  + *for* $i <- 2 dots n$
    + $"dp"_i <- "dp"_(i - 1) + "dp"_(i - 2)$
]

#pseudocode-list()[
  - $#smallcaps[C]""(n, k)$
    + *if* $k = 0$
      + *return* 1
    + *if* $n = 0$
      + *return* $0$
    + *return* $#smallcaps[C]""(n - 1, k - 1) + #smallcaps[C]""(n - 1, k)$
]
#pseudocode-list()[
  + $"dp" <- {-1, dots}$
  + $forall i med "dp"_(i, 0) <- 1$
  + $forall i > 0 med "dp"_(0, i) <- 0$
  + *for* $i <- 1, dots, n$
    + *for* $j <- 1, dots, k$
      + $"dp"_(n, k) <- "dp"_(n - 1, k - 1) + "dp"_(n - 1, k)$
]

#pseudocode-list()[
  - $f""(L, R)$
    + *if* $R - L <= 1$
      + *return* $R - L$
    + $"res" <- +infinity$
    + *for* $M <- L, dots, R$ 
      + $"res" <- min("res", f(L, M) dot f(M, R))$
    + *return* $"res"$
]
#pseudocode-list()[
  + $"dp" <- {-1, dots}$
  + $forall i med "dp"_(i, i + 1) <- 1$
  + $forall i med "dp"_(i, i) <- 0$
  + *for* $"len" <- 0, R - L$ 
    + *for* $l <- 0, dots, L$
      + $r <- l + "len"$
      + *for* $m <- l,dots,r$
        + $"dp"_(l, r) <- min("dp"_(l, r), "dp"_(l, m) dot "dp"_(m, r)))$ 
]
== #theormin Перебор с запоминанием и динамика (subsetsum).
Есть массив $a$, выбрать такие элементы что $sum a_i = S$. Перебор за $O(2^n)$

#pseudocode-list()[
  + _Go_ ($i, "sum"$):
    + *if* $i = n$
      + *return*
    + #text(red)[*if* $"mem"[i, "sum"]$]
      + #text(red)[*return*]
    + #text(red)[$"mem"[i, "sum"] arrow.l 1$]
    + *if* $"sum" > S$
      + *return*
    + _Go_ ($i + 1, "sum"$)
    + _Go_ ($i + 1, "sum" + a_i$)
]

Добавили в перебор #text(red)[мемоизацию] -- $O(n dot S)$

#pseudocode-list()[
  + $"dp"_(i, "sum")$
  + $"dp"_(0, 0) <- 1$
  + *for* $i <- 0, dots, n$
    + *for* $s <- 0, dots, S$
      + *if* $"dp"_(i, s)$
        + $"dp"_(i + 1, s + a_i) <- 1$
]
== Задача "калькулятор": из 1 получить $n min$ числом операций $x -> 2x, 3x, x+1$.
#pseudocode-list()[
  + $forall i med "dp"_i <- +infinity$
  + $"dp"_1 <- 0$
  + *for* $x <- 1, dots, n - 1$
    + $"dp"_(2x) <- min("dp"_(2x), "dp"_x + 1)$
    + $"dp"_(3x) <- min("dp"_(3x), "dp"_x + 1)$
    + $"dp"_(x + 1) <- min("dp"_(x + 1), "dp"_x + 1)$
]

#remark()[
Из числа $1$ получить число $N$. Можем делать:
- $x + 1$
- $2 dot x$
- $3 dot x$

Шаг $f_x = min(f_(x + 1), f_(2x), f_(3x)) + 1$, инициализация $f_n = 0, f_(> n) = + infinity$
]
== Задача "кролик": число путей кролика $x → x+3 dots x+5$ так, что кролик не попадает в дырку.
#pseudocode-list()[
  + $forall i med "dp"_i <- 0$
  + $"dp"_0 <- 1$
  + *for* $i <- 0, dots, n$
    + *for* $l <- 3, dots, 5$
      + *if* $not "hole"_(x + l)$
        + $"dp"_(x + l) <- "dp"_(x + l) + "dp"_x$
]
#fixme()
== Динамика вперёд, назад, ленивая, граф динамики. Две версии: $"dp"[v]$ : $s ~> v$ и $"dp"[v] : v ~> t$.
- Динамика вперед --- знаем ответ для текущего состояния, обновляем (релаксируем) значение для "следующих" состяний
- Динамика назад --- знаем ответ для всех "предыдущих", считаем для текущего
- Ленивая --- рекусия с мемоизацией, не вызываемся от "ненужных" состояний
- Граф динамики --- какие состояния от каких зависят (это видимо DAG)
#fixme()
== Динамика, как задача на ацикличном графе: число путей в графе, min/max путь. Решение для графа в явном виде.
Найти максимальный путь $s -> t$ в DAG
- `dp[v]` --- максимальное расстояние $v -> t$
- `dp[t] = 0`
- `dp[v]` $ = max_((v, u) in E) w_(v u) + "dp[u]"$
- `dp[s]` --- ответ
#pseudocode-list()[
  - `@cache`
  - $#smallcaps[MaxPath]""(v)$
    + *if* $v = t$
      + *return* 0
    + *return* $max_((v, u) in E) w_(u v) + #smallcaps[MaxPath]""(u)$
]

Динамика назад:
#pseudocode-list()[
  + *for* $v <- t - 1, dots, s$ --- в порядке топсорта
    + $"dp"_v <- max_((v, u) in E) w_(u v) + "dp"_u$
]

Динамика вперед:
#pseudocode-list()[
  + *for* $v$
    - $"dp"_v$ уже посчитан
    + *for* $(u, v) in E$
      + $"dp"_u <- max("dp"_u, w_(u v) + "dp"_v)$
]

Можно искать $min$. Можно искать количество путей, заменим $max$ на $sum$ и убрав веса.

Что такое решение для графа в явном виде? #fixme()
== #theormin НВП за $cal(O)(n^2)$.
- `dp[i]` --- наибольшая длины НВП, заканчивающаяся в $a_i$
- $"dp"_i = max_(j < i \ a_j < a_i) "dp"_j + 1$
- Ответ $"argmax"_i "dp"_i$
- $cal(O)(n^2)$
== НВП за $cal(O)(n log n)$.
Будем хранить $d_i$ --- минимальный элемент последовательности длины $i$. Делать по нему бин-поиск вместо перебора всех предыдущих.
#pseudocode-list()[
  + $d_0 <- -infinity$
  + $forall i d_i <- infinity$
  + $"res" <- 0$
  + *for* $i <- 1 dots n$
    + $j <- "upper_bound"(d, a_i)$
    + *if* $d_(j - 1) < a_i and a_i < d_j$
      + $d_j <- a_i$
      + $"res" <- max ("res", j)$
]
== НОП за $cal(O)(n^2)$.
- `dp[i][j]` --- НОП для первых $i$ символов массива $a$ и первых $j$ символов $b$ 
- $"dp"_(i, j) = cases("dp"_(i - 1, j - 1) + 1"," & a_i = b_j, max("dp"_(i - 1, j), "dp"_(i, j - 1)))$
== #theormin Рюкзак (subsetsum и knapsack) за $cal(O)(n S)$.
Выбрать такие элементы что $sum a_i <= S$ и $sum "cost"_i -> max$.

$
f[i, "sum"] = max cases(
  f[i + 1, "sum"] quad & ,
  f[i + 1, "sum" + a_i] + "cost"_i \, quad & #text(font: "DejaVu Serif")[_if_] "sum" + a_i <= S
)
$
== Восстановление ответа на примерах НВП, НОП, рюкзак: со ссылками, без ссылок.
- В НВП можем запоминать предыдущий индекс (тот в котором был максимум)
- В НОП также можем запоминать. Но можем просто пройти обратно, находя переходы если
  1. $"dp"_(i, j) = "dp"_(i - 1, j - 1) + 1$
  2. $"dp"_(i - 1, j) = "dp"_(i, j - 1)$
  2. $"dp"_(i, j - 1) = "dp"_(i - 1, j)$
- В рюкзаке можно аналогично пройти обратно по dp. Либо хранить ссылку на предыдущее состояние динамики (взяли или не взяли предмет) #fixme()
== _Рюкзак с линией памяти. Версии рюкзака, когда предмет можно брать $1$ или $+infinity$ раз._
#solution()[
  $f(k, s)$ --- какую максимальную цену можно набрать из первых $k$, с максимальным весом $S$.
  $ f(k + 1, s) = max cases(
    f(k, s),
    v_k + f(k, s - w_k) quad "," s >= w_k
    )
  $

  Как использовать $cal(O)(S)$ памяти:
  1. Хранить только текущую и предыдущую строчку
  2. Можем использовать один массив и идти по нему слева направо по увеличению $s$.
]
#solution()[
  $ f(k + 1, s) = max cases(
    f(k, s),
    v_k + f(k + 1, s - w_k) quad "," s >= w_k
    )
  $
]
== Рюкзак с bitset.
Хотим выбрать предметы $a_i$ что $sum a_i = S$. Умеем решать за $O(n dot S)$.

#align(center, diagram(spacing: 1em, $
  "" & [i + 1, x] \
  [i, x] edge("ur", ->) edge("dr", ->) & "" \
  "" & [i + 1, x + a_i]
$))

#pseudocode-list()[
  - $"is"[0] <- 1$
  - *for* $i$
    - *for* $x$
      - *if* $"is"[x] and "is"[x + a_i] != 1$
        - $"is"[x + a_1] <- 1$
        - $p[x + a_i] <- i$
]

Сделаем за $O(frac(n dot S, w))$, где $w$ -- размер машинного слова.

#pseudocode-list()[
  - *for* $i arrow.t$
    - `is |= is << ` $a_i$
]
где `is` -- битсет

#section[Динамика-2]
== #theormin DP по дереву: размер поддерева, глубина.
Посчет размера и глубины --- DP. #fixme()
== DP по дереву: max-independent-set, _паросочетание_.
Максимальное независимое множество (Max-Independent-Set (Max IS))
#definition[
  *IS* -- множество попарно несвязных вершин (нет ребра)
]


Можем взять или не взять корень:
- Если не берем, то сумма динамики по детям
- Если берем, то сумма динамики, при том что не берем корень ребенка
$f_(0, v)$ -- не берем корень, $f_v$ -- можем брать можем не брать
$ f_(0, v) = sum_x f_x $
$ f_v = max cases(sum_x f_(0, x) + 1, f_(0, v)) $
== DP на подотрезках: перемножение матриц $A_1, dots, A_n$ за min время.
Нужно перемножить массив матриц: Перебираем последнее действие (если умноженные матрицы справа и слева).
$ f_[0, n) = min_i f_[0, i) + f[i, n) + a_0 dot a_i dot a_n $
#remark()[
  Умеем умножать матрицы $n times m$ и $m times k$ наивно за $m dot n dot k$. Минимизируем количество вариантов.
]
== Выбор состояния-функции: версия рюкзака $"sumWeight"["sumCost", i]$.
Считали $"maxCost"_(i, sum W )$. Если $w_i <= 10^9, S <= 10^9, "cost"_i <= 10$ то проиграем. Поэтому будет считать так $"sumW"_(i, sum "cost") -> min$. Тогда ответ на задачу: $max sum "cost"$ для $"sumW"_(n, sum "cost") <= S$.
== #theormin Хранение множеств масками. Общий принцип.
Очев.

Перебирая маски в порядке увеличения маски будем перебирать множества так, что все подмножества каждого множества уже пройдены.
== _Операции с множествами за $cal(O)(1)$ : $x in A, A inter B, A union B, A \\ B, A subset.eq B, {0, 1, dots, n−1}$, размер._
- $A union B$ --- `A | B`
- $A inter B$ --- `A & B`
- $A \\ B$ --- `A & ~B`
- ${i}$ --- `1 << i`
- $|A|$ --- количество единиц. В C++ есть интризики для подсчета #fixme()
- $A subset.eq B equiv A inter B = A$
- $x in A equiv {x} inter A != emptyset$

== _Операции с множествами за $cal(O)(1)$: найти младший единичный бит._
#solution()[
  `n ^ (n & (n - 1))`
]
== #theormin Гамильтонов путь за $O(2^n n^2)$.
#definition[
  *Гамильтонов путь* -- путь, который проходит по всем вершинам ровно один раз
]

Перебор:
- Начальная вершина $v$
- Множество вершин, в которые уже заходили
Построим динамику на основе этого перебора

#pseudocode-list()[
- *for* $v$
  - $"is"_({v}, v) <- 1$
  - *for* $"mask" = 0..2^n - 1$
    - *if* $"is"_("mask", v)$
      - *for* $x in g_v$
        - *if* $x in.not "mask"$
          - $"is"_("mask" union {x}, x) <- 1$
]
Это за $O(2^n dot n^2)$

== Гамильтонов путь за $O(2^n n)$.
Будем хранить в $"dp"["mask"]$ множество вершин на которые может заканчиваться путь.
#pseudocode-list()[
  + *for* $"mask" arrow.tr$
    + *for* $z$
      + *if* $z in.not "mask" and g^(-1)_z inter "dp"_"mask" != emptyset$
        + $"dp"_("mask" union {z}) <- "dp"_("mask" union {z}) union {z}$
]
Где $g^(-1)_z$ --- множество ребер из которых можно придти в $z$. Восстанавливать ответ можно идя от полной маски, убирая вершины в которых может кончаться путь.

== _Задача SetCover: покрыть множество B минимальным числом $A_i$. Решение за $O(2^(|B|) m)$._
$"dp"_(i, m)$ --- минимальное количество множеств $A_i$ на префиксе, которыми покрывается множество, представленное маской $m$. Динамика вперед. #fixme()
== Пребор пар "множество, подмножество" за $3^n$.
  Почему $O(3^n)$: Перебираем состояние вершины $in.not A, in A and in.not B, in B$:
  ```cpp
  for (auto A = 1; A < pow(2, n); A++) {
    for (auto B = A; B > 0; B--, B &= A) {
      if (is[B]) {
        k[A] = min(k[A], 1 + k[A & ~B]);
      }
    }
  }
  ```
== Решение задачи Vertex Coloring за $cal(O)(3^n)$.
Найти минимальное число цветов, которым можно раскрасить граф. Переберем какие вершины покрасить в цвет 1, какие в цвет 2, и т.д. Предпосчитаем $"is"_A$ -- независимо ли множество вершин $A$.
- Пусть уже покрасили множество вершин $A$, будем хранить $k_A$ -- минимальное количество цветов.
- Выберем подможество $A$
  #pseudocode-list[
    - *for* $A$
      - $display(k_A <- min_(B subset.eq A \ "is"_B = 1) 1 + k_(A \\ B))$
  ]
== DP по профилю: замощение доминошками за $O(4^n m)$.
- Первые $i$ столбцов заполнены, а $i + 1$ стобце могут торчать доминовки из предыдущего. Столбец $i + 1$ можно закодировать с помощью маски. $"dp"_(i, "mask")$ --- количество способов замостить.

#pseudocode-list()[
  + *for* $i arrow.tr$
    + *for* $"mask"$
      + *for* $"mask"'$
        + *if* $"check"("mask", "mask"')$
          + $"dp"_(i + 1, "mask"') <- "dp"_(i + 1, "mask"') + "dp"_(i, "mask")$
]

Как сделать check --- проверять что из одной маски можно получить другую на "следующем уровне", поставив "вертикальные" доминошки.
#pseudocode-list()[
  + *if* $m_1 inter m_2 != emptyset$
    + *return* _false_
  + $m <- m_1 union m_2$
  + *return* $d_m$
]
Где $d$ --- предпосчитанный предикат, который истинен если все дырки в маске четной длины. Можно предпосчитать за $cal(O)(2^n dot n)$.

#section[Жадность]
== #theormin Задача. Непрерывный рюкзак. Жадное решение.
У каждого предмета также есть стоимость и вес, но предметы можно пилить. Можно отсортировать в порядке $frac("cost"_i, w_i)arrow.b$.
#proof()[
  Пусть есть другой оптимальный ответ. Посмотрим на разность количества предметов в нем и в нашем решении. Можем сделать оптимальный отве не хуже, получим наше решение.
]
== Задача. Выполнение всех заданий к дедлайнам. Строгое доказательство корректности.
Есть задачи, у которых есть дедлайн $d_i$ и время выполнения $t_i$. Можно ли выполнить все задачи вовремя и какой в этом случае нужно выбрать порядок выполнения.

Нужно выполнять по увеличению дедлайна.
#proof()[
  Возьмем две подряд идущие задачи $A$ и $B$. Предположим что $d_B < d_A$ (это инверсия), тогда можем безопасно поменять местами эти задачи. Иначе если $d_A < d_B$ (это не инверсия), то может не получиться поменять их местами. Получается что если существует какое-то оптимальное решение, то можем избавится в нем от всех инверсий, и тогда задачи будут идти в порядке увеличения дедлайна.
]
== Задача. Покрыть точки на прямой минимальным числом отрезков длины 1.
Смотрим самую левую непокрытую точку, ставим отрезок туда его левым концом.
#proof()[
  Пусть есть другой оптимальный ответ. Можем двигать отрезки в нем вправо, хуже не станет. Получим наш ответ.
]
== Задача. Выбрать максимальное число непересекающихся отрезков на прямой.
_Решение_: Рассомтрим ответ: Самый левый отрезок это отрезок это отрезок с минимальным $R_i$ (потому что если есть другой, то можем взять его и ответ не ухудшится). Сортировка по правому концу.
== #theormin Метод событий на прямой: для каждой точки узнать, сколькими отрезками она покрыта.
_Решение_: Идем слева направо по прямой, может произойти:
- Отрезок начался -- увеличиваем кол-во отрезков на 1
- Отрезок кончился -- уменьшаем кол-во отрезков на 1
- Встретили точку -- знаем количесвто отрезков
== Метод событий на прямой: жадный выбор заявок для $k$-аудиторий (без док-ва).
_Решение_: Идем слева направо обрабатываем события:
- Встретили левую границу отрезка -- берем этот отрезок. Если больше не можем брать, выкидываем отрезок с наибольшей правой границей.
== Задача: max-independent-set в деревьях, жадное решение.
_Решение_: Возьмем листься, по индукции (без листьев и их родителей) возьмем листья и т.д.
#proof()[
  Пусть есть другое оптимальное решение. Пусть есть лист который не включен. Значит его предок включен, иначе могли бы улучшить ответ. Заменим в этом ответе выбранного предка на лист, ответ не ухудшится, но это будет больше похоже на наш ответ. Значит наш ответ тоже оптимален.
]
== Хаффман. Алгоритм кодирования, декодирования.
Архивация: $"text" mapsto f("text")$, где $f$ обратима.
1. Не существует идеального архиватора: Рассмотрим все битовые строки длины $k$. Их всего $2^k$. Предположим что все строки получилось сжать, то длина сжатого $0, ..., k-1$. Значит их всего $2^0 + 2^1 + dots 2^k < 2^k$, значит какой-то текст не сожмется.

Хотим каждому символу дать битовый код. $A -> 0, B -> 11, C -> 10$. Можем однозначно декодировать если никакой код не является префиксом другого кода. Для каждого символа есть количество раз, которое она встречается в тексте $"cnt"_A$. И соответсвие символов, коду $"code"_A$. 
$ sum_A |"code"_A| dot "cnt"_A -> min $

Как архивируем:
- Выписываем коды
- Выписываем текст, заменяя символы на их код

Как разархивируем:
- Прочитали коды, сложили в бор:
- Идем по тексту и спускаемя по дереву, пока не дойдем по символа, и т.д.

Как построить оптимальные коды: Достаточно знать только количество для каждого символа. Возьмем две самые маленькие $"cnt"_i, "cnt"_j$ и объединяем их, подвешивая их к одному родителю. По получившемуся дереву определяем коды для символов.
== Хаффман. Хранение дерева/частот.
В очереди с приоритетами (куча по максимуму) будем хранить частоты, доставать две объединять их и пихать в очередь сумму их частот.
== Хаффман. Доказательство минимальности.
#proof()[
  Рассмотрим два символа таких что их частоты $f_2 < f_1$ и глубина в дереве $d_2 < d_1$. Рассмотрим как поменяется длина кода если поменяем их местами
  $ (f_2 d_2 + f_1 d_1 + sum dots) - (f_1 d_2 + f_2 d_1 + sum dots) = d_2 (f_1 - f_2) + d_1 (f_1 - f_2) = \
    = undershell((d_2 - d_1), <0) undershell(f_1 - f_2, > 0) < 0
  $
  Получается что символ с большей частотой должен находиться выше. Также не может быть листа у которого нет соседа. Можем убрать лист и сказать что его предок теперь кодирует этот символ. Получается что у всех листов есть сосед (брат). Получается что на самой большой глубине есть самый редкий символ и при этом у него есть сосед со второй минимальной частотой.

  Скажем что эти два символы теперь представляют один символ. Посмотрим как измениться код, когда перестали различать эти два символа
  $ (sum f_i d_i + f_1 d + f_2 d) - (sum f_i d_i + (f_1 + f_2) (d - 1) = f_1 + f_2 $

  Допустим есть оптимальное дерево для алфавита $Alpha \\ {s_1, s_2} union {x}$, где $s_1, s_2$ --- самые редкие символы, а $x$ --- новый символ с частотой $f_1 + f_2$. Пусть есть оптимальное дерево для $Alpha$, сожмем две самые глубокие вершины, получим какое-то кодирующее дерево для алфавита $Alpha \\ {s_1, s_2} union {x}$. Если оно не оптимальное то существует оптимальное дерево кодирующее этот алфавит. У неоптимального длина $L'$, у оптимального $L_"opt"'$. Если в нем расщепим вершину $x$, то получим длины кодов для $Alpha$: $L' + (f_1 + f_2) > L_"opt"' + (f_1 + f_2)$. Получили лучшее дерево для исходного алфавита, но это значит что исходное дерево было неоптимальное. Противоречие. Значит если объединим символы, то получим оптимальное дерево.

  По индкукции получим что строим оптимальное дерево.
]
#section[MST и DSU]
== Лемма о безопасном ребре.
Пусть уже выбрали какие-то ребра. Если выберем произвольный разрез этого графа, который не пересекаюти выбранные ребра, то выбрав минимальное ребро из изначального графа, которое пересекает разрез, все еще сможем достроить этот подграф до MST.
#proof()[
  Путь достроили до MST но минимальное ребро из разреза не в нем. В MST есть единственный путь между двумя любыми вершинами и какое-то ребро в нем пересекает этот разрез. Построим новый граф удалив это ребро и добавив минимальное между этими двумя вершинами. Путь останется, получим новое MST, которое не хуже.
]
== MST, алгоритм Прима, доказательство.
Возьмем ребро с минимальным весом, добавим его в дерево. Возьмем минимальные ребро $e$ их тех, что имеют конец в вершине, которая уже в дереве $A$.

#statement()[
  $exists  T : e in T$, где $T$ -- MST
]

#pseudocode-list()[
- *while* $A != V$
  - Взять минимальное по весу $e$, имеющее конец в вершине в $A$
]

#proof()[
  Рассмотрим $T$. Пусть $e in.not T$. Если добавим в $T$ ребро $e$, получим цикл. Ребро $e$ торчит из $A$ в $V \\ A$. Значит есть другое ребро $e'$, соединяющее $A$ и $V \\ A$. Т.е. есть другое дерево $T \\ {e'} union {e}$ которое не хуже.
]

#pseudocode-list(title: "Алгоритм Прима")[
- $A <- {1}$
- $x <- 1$
- *for* $n - 1$
  - *for* $e := x -> v$
    - Улучшить $"minW"_v$ до $w_e$
  - Выбрать $x in.not A: "minW"_x = min$
]

== #theormin MST, алгоритм Прима, реализации за $V^2, E log V$.
#remark()[
  #pseudocode-list()[
    + $p <- {-1, dots, -1}$
    + $d <- {-infinity, dots, -infinity}$
    + $d_0 <- 0$
    + $w <- 0$
    + *repeat* $|V|$ *times*
      + $v <- op("argmin", limits: #true)_(v: d_v != -infinity) d_v$
      + $w <- w + d_v$
      + $d_v <- -infinity$
      + *for* $u: v -> u$
        + *if* $d_u > w_(u v)$
          + $d_u <- w_(u v)$
          + $p_u <- v$
  ]
]

Это $cal(O)(|V|^2 + |E|)$. Используем кучу для получения минимума и делаем `decreaseKey` --- $cal(O)(E log V)$.
== MST, алгоритм Прима, реализации за $E log_(E/V) V$ ($d$-куча).
Используем $d$-кучу вместо обычной. В $d$-куче `decreaseKey` работает за $cal(O)(log_d V)$. Получим время $V d log_d V + E log_d V$. Выгодно взять $d = max(E / V, 2)$, тогда слагаемые уравняются. Получим время $E log_(E / V) V$. Если граф плотный $V^2 ~ E$, то работает за $cal(O)(E)$, если же $V ~ E$, то $cal(O)(E log V)$.
== MST, алгоритм Краскала, доказательство.
1. Отсортируем ребра по возрастанию веса
2. Будем жадно брать ребра, если не образуется цикл, поддерживая множества вершин, которые уже соединены.
Нужно уметь быстро определять компоненту связности по вершине (из текущего остова).
#proof()[
  Инвариант: все выбранные ребра можно достроить до MST.

  1. Если ребро соединяет две вершины из одного множество, то оно образует цикл. Его не может быть в MST
  2. Если ребро соединяет две вершины из разных множеств, то можно построить разрез и это ребро будет его пересекать. По лемме о безопасном ребре инвариант сохраниться.
]
== MST, алгоритм Краскала, реализация, время работы.
#pseudocode-list()[
- *for* $e = a -> b arrow.tr$
  - *if* $"Comp"(a) != "Comp"(b)$
    - $"Add"(e)$
]
== DSU. Система Непересекающихся Множеств. Интерфейс.
- $"get"(a)$ -- просто берем из списка номер множества $O(1)$
- $"join"(a, b)$ -- перекрашиваем меньшее из множест: перекидываем все вершины, обновляем номер множества для вершин. В сумме за $O(n log n)$.
== DSU на списках. join-ы за $n log n$, get за $1$.
Для каждой вершины храним номер множества в котором она находится, а также для каждого множества храним список вершин.
#proof[Для каждой вершины при перекидывании, размер множества в котором она находится увеличивается минимум вдвое. Значит для каждой вершины количество перекидываний не больше $log n$]
== DSU на деревьях, меньшее к большему $==>$ join и get за $log n$.
Храним родителя для каждой вершины $v$ в $p_v$.
- $"get"$ -- поднимаемся до корня дерева
- $"join"$ -- поднимаемся в обоих деревьях до корня, подвешиваем меньшее к большему

#pseudocode-list(title: "join(a, b)")[
- $a <- "get"(a)$
- $b <- "get"(b)$
- *if* $"size"_a > "size"_b$
  - $"swap"(a, b)$
- $p_a <- b$
- $"size"_b <- "size"_b + "size"_a$
]

#proof()[
  В худшем случае `find` работает за высоту дерева. Покажем что $n >= 2^h$. По индукции. База $h = 0, 1 >= 2^0$. Если $h > 0$, рассомтрим как оно получилось: подвесили куда-то дерево высоты $h - 1$. В этом дереве $>= 2^(h - 1)$ вершин по ИП, во втором тоже хотя бы $>= 2^(h - 1)$, т.к. подвешиваем меньшее к большему, получается в новом дереве $>= 2 dot 2^(h - 1) = 2^h$ вершин. 
]
== DSU на деревьях, эвристика сжатия путей.
Сжатие путей: переподвешиваем детей сразу к корню
```cpp
int get(int v) {
  return v == p[v] ? v : (p[v] = get(p[v]));
}

void join(int a, int b) {
  p[get(a)] = b;
}
```
== DSU на деревьях: две эвристики с помощью одного доп массива.
#theorem()[
  - Если используем только ранговую эвристику, то $cal(O)(log n)$
  - Если используем только сжатие путей, то $cal(O)(log n)$
  - Если обе эвристики, то $cal(O)(log^* n)$. На самом деле за $alpha(n)$, где $alpha$ --- обратная функция Аккермана.
]

Если вершина является корнем, то интересен только ее размер. Если же не корень, то интересно только какой предок. Получается можно хранить вместо двух массивов $p$ и $"size"$, то можем хранить только один массив где $p_i < 0$, то $i$ это корень и $- p_i$ это размер дерева, если $p_i > 0$, то $p_i$ это предок $i$.

#section[Графы. DFS.]
== Определения: граф, смежность, инцидентность, орграф, неорграф.
#remark()[
  - $G = chevron.l V, E chevron.r$, $n = |V|, m = |E|$
  - Кратные ребра -- несколько ребер между двумя вершинами
  - Инцидентность -- ребро и каждая его вершина
]

#remark()[
  Смежность
  - Ребра инцидентные одной и той же вершине смежны
  - Вершины инцидентные одному и тому же ребру смежны
]
== #theormin Хранение графов: матрица смежности, списки рёбер.
- Матрица инцидентности. Матрица $E times V$, где в $m_(e, a)$ стоит $1$ если ребро $e$ инцидентно вершине $a$. Почти не используется
- Матрица смежонсти. Матрица $V times V$, где $m_(a, b)$ стоит $1$ если есть ребро $a -> b$.
  + Можно проверять существование ребра за $cal(O)(1)$
  + Требуется $cal(O)(V^2)$ памяти
  + Чтобы перебрать ребра одной вершины нужно $cal(O)(V)$ времени
- Списки смежности. Для каждого ребра храним его ребра.
  + $cal(O)(V + E)$ памяти
  + $cal(O)("deg"(v))$ чтобы перебрать ребра вершины $v$
  + $cal(O)(1)$ проверка существования ребра, если хранить в `unordered_set`, но другие операции будут медленнее
== Хранение графов: set-ы рёбер, сравнение всех трёх способов.
См. выше #fixme()
== #theormin dfs: проверка достижимости, поиск компонент.
Очев.
== dfs: поиск пути, обратный ход рекурсии.
Когда делаем DFS сохраняем для каждой вершины предка. Таким образов после завершения будем иметь дерево, для каждой вершины есть единственный сохраненный путь по корня, можем видимо искать какие-то пути между двумя вершинами (через корень). Что такое обратный ход рекурсии? #fixme()
== dfs и DAG: что такое цикл? topsort, времена входа-выхода.
#statement()[
  Топсорт это порядок вершин по уменьшению $t_"out"$.
]
#proof()[
  Рассмотрим ребро $a -> b$. Если зайдем сначала в $a$, то $t_"out"^a > t_"out"^b$. Если сначала зайдем в $b$, то не можем существовать другого путя из $b$ в $a$, значит выйдем из $b$ и только потомзайдем $a$, тогда тоже $t_"out"^a > t_"out"^b$.
]
== dfs: поиск цикла в орграфе.
- Будем красить вершины (не заходили, зашли, вышли/белая, серая, черная).
- Будем поддерживать стек рекурсии (хранить там вершины). Когда встретили серую вершину, то нашли цикл. Путь лежит в стеке.
== _dfs: поиск цикла в неорграфе._
В неорграфе есть только обратные ребра (они же прямые), поэтому можно красить вершины: если пришли в посещенную то это и есть цикл (обратное ребро). В остальном см. выше. #fixme()
== _dfs: покраска в два цвета._
Жадно красим. Меняем цвет при переходе к смежной вершине. Если встретили противоречие то покрасить нельзя. #fixme()
== Дерево dfs: классификация рёбер.
- Есть ребра по которым прошли --- *ребра обхода*
- *Прямые ребра* --- из предка в потомка
- *Обратные ребра* --- из потомка в предка
- *Перекрестные ребра* --- все остальные? между вершинами в разных поддеревьях
== Дерево dfs: неорграф не содержит перекрёстных рёбер.
#statement()[
  В неорграфе нет перекрестных ребер
]
#proof()[
  Рассомтрим дерево обхода. Не можем обнаружить перекрестное ребро $v -> u$ т.к. тогда мы бы по нему прошли когды были в вершине $u$.
]
== Компоненты сильной связности. Определение, простая проверка сильной связности.
#definition()[
  Вершины сильно связаны если существует путь $a ~> b$ и $b ~> a$.
]

Как искать компоненту сильной связности вершины $v$:
- Запустим DFS из вершины $v$
- Запустим DFS из вершины $v$ на транспонированном графе
- Первый DFS обойдет все вершины достижимые из $v$ --- $"used"$. Второй обойдет все вершины из которых достижима $v$ --- $"used"^T$. Тогда компоненты сильной связности это $"used" inter "used"^T$.
== Компоненты сильной связности: выделение всех компонент, построение конденсации.
#remark()[
  Есть КСС $A$ и $B$. Пусть $T_"out" (A) = max_(v in A) t_"out" (v)$. Если есть ребро $A -> B$
  - Если сначала зашли в $A$, то $T_"out"(A) > T_"out"(B)$. Т.к. выйдем из $A$ только когда обойдем все вершины в $B$. 
  - Если сначала зашли в $B$, то аналогично топсорту зайдем в $A$ только когда выйдем из $B$, и тогда тоже $T_"out"(A) > T_"out"(B)$.
]
- Обойдем все вершины DFS'ом. 
- Запустим транспонированный DFS из вершины с самым большим временем выхода. Таким образов сможем найти "первую" КСС. Не зайдем ни в какие другие т.к. из них не может быть ребер в $A$ в транспонированном графе.
- "Выпилим" КСС $A$ из графа и опять запустимся из вершины с максимальным времен выхода.
#pseudocode-list()[
  + *for* $v in V$
    + *if* $not "used"_v$
      + $"dfs"(v)$
  + $c <- 0$
  + *for* $v in t_"out" arrow.b$
    + *if* $not "used"_v$
      + $"dfs"^T (v)$
      + $c <- c + 1$
]
В dfs можем запомнить $c$ для каждой посещенной вершины. Это будет ее КСС.
== _2-связность: поиск мостов и компонент (dp по дереву)._
Рассмотрим дерево обхода. Заметим что если ребро --- мост, то из его поддерева нет ребер выше нижней вершины этого ребра. Считаем динамику (?) $"dp"_v$ --- для каждой вершины минимальное время входа вершин в которые можем из нее попасть. Тогда можем проверить что ребро $a -> b$ мост если $"dp"_b > t_"in" (a)$.

Если удалим мосты то граф распадется на компоненты связности. Видимо можем их найти не переходя по мостам. #fixme()
== 2-SAT.
#definition()[
  2-SAT --- конъюктивная нормальная форма, но в каждой скобке строго 2 переменные.
  $ (x_1 or x_2) and (not x_3 or x_4) and dots $.
  Нужно определить существует ли набор значений переменных, чтобы вся формула вычислялась в истинну.
]
Для каждой переменной $a$ создадим две вершины $a$ и $not a$.
#example()[
  $ (a or b) and (c or b) and (not a or not c) $
  #align(center)[
    #diagram(spacing: 1em, $
      a edge("drr", ->) & b & c edge("dll", ->)  \
      not a edge("ur", ->) & not b  edge("ul", ->) edge("ur", ->) & not c edge("ul", ->)
    $)
  ]
  Это граф импликации.
]
Хотим раскрасить вершниы в два цвета так что если покрасили одну вершину в цвет `true`, то все вершины достижимые из нее должны быть покрашены в этот же цвет `true`. При этом $a$ и $not a$ должны быть покрашены в разные цвета.

В этом графе могут быть КСС в которых все переменные либо true либо false. Сконденсируем граф. Сделаем топсорт. В этой сортировке где-то будет $x$ а где-то $!x$. Нужно назначить одной из них $1$, другой $0$. Будем присваивать той что правее $1$, а той что левее $0$.

#remark()[
  Если есть ребро $x -> y$, то должно быть ребро $not y -> not x$.
]

#proof()[
  Пусть получили противоречие: из переменной $x$ есть ребро $y$ и назначили $x = 1$, а $y = 0$. Но если в $x$ назначили $1$, то левее должно быть $not x = 0$. И соответственно $not y = 1$. Но тогда должно быть ребро $not y -> not x$, но это ребро направлено справа на лево, что противоречит тому что сделали топсорт.
]

#remark()[
  В алгоритме построения конденсации нумеровали компоненты в порядке топсорта. Т.е. если $C[x] > C[not x]$, то $x = "true"$, если $C[not x] > C[x]$, то $x = "false"$, если же $C[not x] = C[x]$, то решения нет.
]

#section[Кратчайшие пути]
== #theormin BFS. Две версии: по слоям; с очередью.
Очев.

- Кратчайший путь в графе с невзвешенными ребрами $w_e = 1$.

#remark()[
  BFS по слоям $A_d$:
  #pseudocode-list()[
    - *for* $i in {0, ...}$
      - *for* $v in A$
        - *for* $e : v -> x$
          - *if* $d_x = -1$
            - $A_(i + 1) <- A_(i + 1) union angle.l x angle.r$
            - $d_x <- i + 1$
  ]
]
== BFS. _Модификация для целых $1-k$ весов за $O(E + k V)$ и $O(E log k)$._
Будем хранить $k$ очередей и брать из первой (с расстояние $1$), когда она заканчивается, то условно делаем ее последней, а на первое место ставим вторую. Т.е. надо делать по модулю $k$. Это за $cal(O)(E + k V)$.

Можем хранить кучу по минимуму с номерами непустых массивов. Тогда будет $cal(O)(E log k)$.

#fixme()
== BFS. _Модификация для вещественных $1-k$ весов._
То же самое, но очереди теперь условные бакеты. Нужно еще поддерживать реальное расстояние до вершин, которые находятся в очередях. И класть потом в соответсвующий бакет. #fixme()

== BFS. _Модификация для целых $0-1$ весов._
Вместо очереди используем деку. Если ребро весит $0$ то пихаем в начало, иначе в конец. Достаем из начала.

#remark()[
  Можно склеить вершины по ребрам веса $0$ (?)
]

#fixme()
== Дейкстра. Алгоритм и доказательство корректности.
#proof()[
  Пусть есть вершины для которых уже знаем расстояние $A$ и все остальные для которых есть кандидат расстояние $V \\ A$. Если взять $v -> u: v in A$ и $u in V \\ A$, есть потонциальное расстояние до $u$ $d_u + w_(v u)$. Покажем что это $d_u + w_(v u)$ это минимальное расстояние до $u$.

  Пусть есть путь короче до $u$ из какой-то вершины $a in A$ до $b in V \\ A$ и от $b$ до $u$. Но тогда $d_a + w_(a b) >= d_v + w_(v u)$.
]
== #theormin Дейкстра. Реализации за $O(n^2)$, $O(m log n)$.
#pseudocode-list()[
  + *for* $n$
    + $v <- "argmin"_v d_v$
    + *for* $v -> x$
      + $"Relax" d_x$
]
Нужна структура данных которая умеем доставать минимум и уменьшать ключ. Тогда работаем за $V dot "ExtractMin" + E dot "DecreaseKey"$.
- На массиве: $V^2 + E dot 1$. Полезно для плотных графов если $E ~ V^2$
- Куча: $V dot log V + E dot log V$. Для неплотных графов.
== Реализация на C++/python: `set`, `prioirity_queue`, `operator<`, `heapq.heappush`.
```cpp
struct V {
    int v, d;
};

void sol() {
    // ...

    vector<int> d(n, -1);
    auto cmp = [](const V& l, const V& r) { return l.d > r.d; };
    priority_queue<V, vector<V>, decltype(cmp)> q;

    q.push({s, 0});
    while (!q.empty()) {
        auto [u, du] = q.top(); q.pop();
        DBG() << u + 1 << " " << du << endl;
        if (d[u] != -1) continue;
        d[u] = du;

        if (u == t) break;

        for (auto e : g[u]) {
            q.push({e.t, d[u] + e.w});
        }
    }

    // ...
}
```
== Дейкстра. Время работы при применении куч: $d$-ичная, VEB, фибоначчиева.
- VEB: $(V + E) dot log log C$
- $d$-куча: $V dot d log_d V + E dot log_d V$
- Фибоначиева: достаем минимум за $log V$, а уменьшение ключа за $1$. $V log V + E dot 1$
== Дейкстра. Двусторонняя вариация алгоритма.
Чаще решаем задачу найти кратчайший путь из $A$ в $B$. Можем параллельно пойти из двух концов. Есть две кучи для $A$ и для $B$, достаем минимум из каждой и идем в ту, которая меньше. Или можно с одной кучей, но храним для каждой вершины откуда мы из нее пришли.

== Алгоритм A∗. Пример применения.
Хотим найти минимальный путь не до всех, а до какого-то $t$. Знаем какое-то примерное расстояние от каждой вершины до $t$. Поменяем ключ кучи в Дейкстре на $d_u + f(u)$, где $f$ --- эта функция оценки.

#example()[
  - Путь между городами. Можно делить на разные слои (магистрали, не магистрали, $dots$)
  - Проплыть между объектами. Точки графа, там где нужно повернуть, лениво добавляем ребра.
]
#remark()[
  - $f(u) <= "dist"(u, t)$
  - $w_(a b) + f(a) >= f(b)$
]
== Алгоритм A∗. Доказательство корректности, оценка времени работы.
#proof()[
  Есть вершины с известным расстоянием $A$. Выбрали новую вершину $u$ с минимальным $d_u + f(u)$. Докажем что расстояние до $u$ найдено корректно. Пусть какая-то другая вершина $a$, пройдя через которую расстояние до $u$ будет меньше: $d_a + f(a) >= d_u + f(u)$.

  Рассмотрим случай когда есть ребро $a -> u$. По свойству $f$: $f(u) + w_(a u) >= f(a)$
  $ f(u) >= f(a) - w_(a u) ==> d_a + f(a) >= d_u + f(u) >= d_u + f(a) - w_(a u) \
    d_a >= d_u - w_(a u) ==> d_a + w_(a u) >= d_u
  $
  Если между $a$ и $u$ несколько ребер, то проделаем это несколько раз (или по "транзитивности"). Получим противоречие что $d_u$ короче чем путь через какой другой $a$.
]

#section[Амортизация]
== _Алгоритм Борувки._
#link("https://neerc.ifmo.ru/wiki/index.php?title=%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%91%D0%BE%D1%80%D1%83%D0%B2%D0%BA%D0%B8")[Neerc Link]

1. Изначально каждая вершина графа $G$ --- тривиальное дерево, а ребра не принадлежат никакому дереву.
2. Для каждого дерева $T$ найдем минимальное инцидентное ему ребро. Добавим все такие ребра.
3. Повторяем шаг 2 пока в графе не останется только одно дерево $T$.


== Skew heap. Алгоритм.
Есть операция `Merge` (Meld), которая объединяет две кучи.
- `Add` -- сделать `Merge` с кучей из одного элемента
- `ExtractMin` -- Удалить корень, сделать `Merge` на его детях

#pseudocode-list("Merge")[
  - #smallcaps[Merge] ($A, B$)
    + *if* $A."value" >= B."value"$
      + $A."right" <- "Merge"(A."right", B)$
      + $"Swap"(A."right", A."left")$ --- Для балансировки
    + *else*
      + $B."right" <- "Merge"(A, B."right")$
      + $"Swap"(B."right", B."left")$
]

Leftist Heap: Будем хранить в нодах еще глубину правого дерева $d$
#pseudocode-list()[
  - #smallcaps[Merge] ($A, B$)
  + *if* $A = emptyset$
    + *return* $B$
  + *if* $B = emptyset$
    + *return* $A$
  + *if* $A.x > B.x$
    + $"Swap"(A, B)$
  + $A.R <- #smallcaps[Merge]""(A.R, B)$
  + *if* $A.R.d > A.L.d$
    + $"Swap"(A.R, A.L)$
  + $A.d <- A.R.d + 1$
  + *return* $A$
]

Глубина этой кучи $cal(O)(log n)$.
== Skew Heap, доказательство: лёгкие и тяжёлые рёбра, потенциал.
#proof("Leftist Heap")[
  Покажем что если спустимся в право по кучи то все элементы выше будут существовать. Тогда получается что если спустились на $k$, то всего элементов в куче $2^k$. Пусть есть дырка где-нибудь, но тогда должны были в какой-то момент поменять местами по условию на глубину.
]

#definition()[
  Легкие/Тяжелые ребра. Ребро тяжелое если в его поддереве больше половины вершин.
]
#lemma()[
  Количество легких ребер не больше $log n$.
]
#proof()[
  При переходе снизу по легкому ребро размер родителя $S$, а поддерева $<= S/2$. Получается размер поддереве удвоится. Удваиваний может быть не больше $log n$.
]
#symb()[$\#L$ --- количество легких ребер]
#proof("Skew Heap")[
  Рассмотрим путь по которому спускалось при Мерже. На пути было $\#L <= log N$ и $\#H = k$. Отработали за $cal(O)(k + log N)$. Введем потенциал $phi = \#"HR"$ --- количество правых тяжелых ребер.

  Когда свопаем тяжелое ребро то потенциал уменьшается на $k$, а когда легкое --- увеличивается на $log N$. Получается $Delta phi <= -k + log N$
]
== DSU. Доказательство $log n$ для сжатия путей.
#proof()[
  Рассмотрим путь по которому прошли. В нем $\#L <= log N$. Рассмотрим какое-то тяжелое ребро у вершины $v$ которе будет перенесено к корню. Получается размер поддерева $v$ уменьшился больше чем в два раза. В жизни каждой вершины таких событий не больше $log N$, т.е. $sum \#H <= N log N$.
]
== DSU. Доказательство $log^∗ n$ для двух эвристик.
#definition()[
  *Крутое* ребро --- если размер (ранг) поддерева $x$, то размер (ранг) всего дерева $2^x$.
]
#lemma()[
  Крутых ребер $\#C <= log^* n$.
]
#statement()[
  Каждое некрутое ребро через $2^(R[v])$ операций станет крутым.
]
#proof([Время работы get, join])[
  Посчитаем суммарное время работы, проссумируем по рангам:
  $ sum_R 2^R N(R) $
  где $N(R)$ --- количество вершин ранга $R$. Заметим что все деревья одного ранга не пересекаются. А также в каждом дереве ранга $R$ количество вершин $>= 2^R$. Поэтому количество деревьев ранга $R$ не больше чем $N / 2^R$. Но тогда
  $ sum_R 2^R dot N / 2^R = N dot sum_R $
  Заменим везде $2$ на $1.5$. Заметим что $1.5^(1.5^x) >= 2^x$ при $x > 3$.
  $ sum_R 1.5^R dot N / 2^R = N sum_R (1.5 / 2)^R = N dot cal(O)(1) $
]
== Амортизационный анализ: метод монеток (ростовщика) на примере вектора.
При `push_back` кладем 2 моентки, когда нужно реаллоцировать тратим монетки, их должно хватить. Если хотим деаллоцировать память в векторе при уменьшении кладем по 4 монетки и на `push_back` и на `pop_back`. #fixme()
== Турнирное дерево (аналог дерева отрезков).
Есть элементы, будем устраивать между ними турнир выбирая минимум (похоже на дерево отрезков). Предположим что у нас $2^k$ элементов, тогда дерево будет ранга $k$. Можем мержить два дерева одного ранга за $cal(O)(1)$.
== Списко-куча: `add`, `merge`, `min` за $cal(O)(1)$ и `extractMin` за амортизированный $cal(O)(log n)$
Списко-куча: Элементы хранятся как связный список. Add и Merge можем делать за $cal(O)(1)$. ExtractMin можно делать за $cal(O)(n)$: храним указатель на минимум, удаляем и проходимся ищем новый минимум.

Будем хранить не элементы а турнирные деревья. Пусть всего элементов в списке $"Roots"$. Тогда время работы ExtractMin $t_1 = "Roots"$. Хотим взять потенциал $phi = "Roots"$, тогда $Delta phi <= -"Roots" + log N$. Когда делаем ExtractMin объединяем деревья одного ранга. Всего деревьев тогда будет $<= log N$.

Будем для каждого ранга хранить ровно один элемент в $"Root"["Rank"]$. При добавлении нового дерева если такой ранг уже был, создаем дерево на один ранг больше.
#pseudocode-list()[
  + *while* $"Root"["Rank"] != emptyset$
    + $V <- "join"("Root"["Rank"] V)$
    + $"Root"["Rank"] <- emptyset$
    + $"Rank" <- "Rank" + 1$
  + $"Root"["Rank"] <- V$
]
